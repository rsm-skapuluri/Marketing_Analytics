[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nSrujith Kapuluri\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nSrujith Kapuluri\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nSrujith Kapuluri\n\n\nApr 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Blog/Project3/index.html",
    "href": "Blog/Project3/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results.\n\n\n\n\n\n\nData\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(haven)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the dataset\ndf &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Display structure of the data\nglimpse(df)\n\nRows: 50,083\nColumns: 51\n$ treatment          &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, …\n$ control            &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, …\n$ ratio              &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 1,…\n$ ratio2             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, …\n$ ratio3             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ size               &lt;dbl+lbl&gt; 0, 0, 3, 4, 2, 0, 1, 3, 4, 1, 4, 2, 0, 1, 0, 4,…\n$ size25             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ size50             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ size100            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sizeno             &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ ask                &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 3, 3, 2, 2, 1, 3, 0, 2, 0, 1,…\n$ askd1              &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ askd2              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ askd3              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ ask1               &lt;dbl&gt; 55, 25, 55, 55, 35, 95, 125, 75, 250, 150, 125, 25,…\n$ ask2               &lt;dbl&gt; 70, 35, 70, 70, 45, 120, 160, 95, 315, 190, 160, 35…\n$ ask3               &lt;dbl&gt; 85, 50, 85, 85, 55, 145, 190, 120, 375, 225, 190, 5…\n$ amount             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gave               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ amountchange       &lt;dbl&gt; -45, -25, -50, -25, -15, -45, -50, -65, -100, -125,…\n$ hpa                &lt;dbl&gt; 45, 25, 50, 50, 25, 90, 100, 65, 200, 125, 100, 5, …\n$ ltmedmra           &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, …\n$ freq               &lt;dbl&gt; 2, 2, 3, 15, 42, 20, 12, 13, 28, 4, 1, 1, 2, 80, 3,…\n$ years              &lt;dbl&gt; 4, 3, 2, 8, 95, 10, 8, 16, 19, 7, 3, 1, 6, 19, 3, 1…\n$ year5              &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, …\n$ mrm2               &lt;dbl&gt; 31, 5, 6, 1, 24, 3, 4, 4, 6, 35, 41, 8, 28, 15, 5, …\n$ dormant            &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ female             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ couple             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ state50one         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ nonlit             &lt;dbl&gt; 5, 0, 3, 1, 1, 0, 0, 4, 1, 4, 4, 1, 1, 4, 0, 3, 6, …\n$ cases              &lt;dbl&gt; 4, 2, 1, 2, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 1, 1, 2, …\n$ statecnt           &lt;dbl&gt; 4.5002995, 2.9822462, 9.6070213, 3.2814682, 2.30201…\n$ stateresponse      &lt;dbl&gt; 0.01994681, 0.02608696, 0.02304817, 0.02066869, 0.0…\n$ stateresponset     &lt;dbl&gt; 0.019502353, 0.027833002, 0.022158911, 0.024702653,…\n$ stateresponsec     &lt;dbl&gt; 0.020806242, 0.022494888, 0.024743512, 0.012681159,…\n$ stateresponsetminc &lt;dbl&gt; -0.001303889, 0.005338114, -0.002584601, 0.01202149…\n$ perbush            &lt;dbl&gt; 0.4900000, 0.4646465, 0.4081633, 0.4646465, 0.52525…\n$ close25            &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ red0               &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, …\n$ blue0              &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, …\n$ redcty             &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, …\n$ bluecty            &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ pwhite             &lt;dbl&gt; 0.4464934, NA, 0.9357064, 0.8883309, 0.7590141, 0.8…\n$ pblack             &lt;dbl&gt; 0.527769208, NA, 0.011948366, 0.010760401, 0.127420…\n$ page18_39          &lt;dbl&gt; 0.3175913, NA, 0.2761282, 0.2794118, 0.4423889, 0.3…\n$ ave_hh_sz          &lt;dbl&gt; 2.10, NA, 2.48, 2.65, 1.85, 2.92, 2.10, 2.47, 2.49,…\n$ median_hhincome    &lt;dbl&gt; 28517, NA, 51175, 79269, 40908, 61779, 54655, 14152…\n$ powner             &lt;dbl&gt; 0.4998072, NA, 0.7219406, 0.9204314, 0.4160721, 0.9…\n$ psch_atlstba       &lt;dbl&gt; 0.32452780, NA, 0.19266793, 0.41214216, 0.43996516,…\n$ pop_propurban      &lt;dbl&gt; 1.0000000, NA, 1.0000000, 1.0000000, 1.0000000, 0.9…\n\n# Display summary statistics\nsummary(df)\n\n   treatment         control           ratio           ratio2      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.6668   Mean   :0.3332   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     ratio3            size           size25           size50      \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :2.000   Median :0.0000   Median :0.0000  \n Mean   :0.2222   Mean   :1.667   Mean   :0.1667   Mean   :0.1666  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :4.000   Max.   :1.0000   Max.   :1.0000  \n                                                                   \n    size100           sizeno            ask            askd1       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.1667   Mean   :0.1667   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     askd2            askd3             ask1             ask2        \n Min.   :0.0000   Min.   :0.0000   Min.   :  25.0   Min.   :  35.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  35.0   1st Qu.:  45.00  \n Median :0.0000   Median :0.0000   Median :  45.0   Median :  60.00  \n Mean   :0.2223   Mean   :0.2222   Mean   :  71.5   Mean   :  91.79  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:  65.0   3rd Qu.:  85.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1500.0   Max.   :1875.00  \n                                                                     \n      ask3          amount              gave          amountchange       \n Min.   :  50   Min.   :  0.0000   Min.   :0.00000   Min.   :-200412.12  \n 1st Qu.:  55   1st Qu.:  0.0000   1st Qu.:0.00000   1st Qu.:    -50.00  \n Median :  70   Median :  0.0000   Median :0.00000   Median :    -30.00  \n Mean   : 111   Mean   :  0.9157   Mean   :0.02065   Mean   :    -52.67  \n 3rd Qu.: 100   3rd Qu.:  0.0000   3rd Qu.:0.00000   3rd Qu.:    -25.00  \n Max.   :2250   Max.   :400.0000   Max.   :1.00000   Max.   :    275.00  \n                                                                         \n      hpa             ltmedmra           freq             years       \n Min.   :   0.00   Min.   :0.0000   Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  30.00   1st Qu.:0.0000   1st Qu.:  2.000   1st Qu.: 2.000  \n Median :  45.00   Median :0.0000   Median :  4.000   Median : 5.000  \n Mean   :  59.38   Mean   :0.4937   Mean   :  8.039   Mean   : 6.098  \n 3rd Qu.:  60.00   3rd Qu.:1.0000   3rd Qu.: 10.000   3rd Qu.: 9.000  \n Max.   :1000.00   Max.   :1.0000   Max.   :218.000   Max.   :95.000  \n                                                      NA's   :1       \n     year5             mrm2           dormant           female      \n Min.   :0.0000   Min.   :  0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:  4.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :  8.00   Median :1.0000   Median :0.0000  \n Mean   :0.5088   Mean   : 13.01   Mean   :0.5235   Mean   :0.2777  \n 3rd Qu.:1.0000   3rd Qu.: 19.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :168.00   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :1                         NA's   :1111    \n     couple         state50one            nonlit          cases    \n Min.   :0.0000   Min.   :0.0000000   Min.   :0.000   Min.   :0.0  \n 1st Qu.:0.0000   1st Qu.:0.0000000   1st Qu.:1.000   1st Qu.:1.0  \n Median :0.0000   Median :0.0000000   Median :3.000   Median :1.0  \n Mean   :0.0919   Mean   :0.0009983   Mean   :2.474   Mean   :1.5  \n 3rd Qu.:0.0000   3rd Qu.:0.0000000   3rd Qu.:4.000   3rd Qu.:2.0  \n Max.   :1.0000   Max.   :1.0000000   Max.   :6.000   Max.   :4.0  \n NA's   :1148                         NA's   :452     NA's   :452  \n    statecnt         stateresponse     stateresponset    stateresponsec   \n Min.   : 0.001995   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.: 1.833234   1st Qu.:0.01816   1st Qu.:0.01849   1st Qu.:0.01286  \n Median : 3.538799   Median :0.01971   Median :0.02170   Median :0.01988  \n Mean   : 5.998820   Mean   :0.02063   Mean   :0.02199   Mean   :0.01772  \n 3rd Qu.: 9.607021   3rd Qu.:0.02305   3rd Qu.:0.02470   3rd Qu.:0.02081  \n Max.   :17.368841   Max.   :0.07692   Max.   :0.11111   Max.   :0.05263  \n                                                         NA's   :3        \n stateresponsetminc     perbush           close25            red0       \n Min.   :-0.047619   Min.   :0.09091   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.001388   1st Qu.:0.44444   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 0.001779   Median :0.48485   Median :0.0000   Median :0.0000  \n Mean   : 0.004273   Mean   :0.48794   Mean   :0.1857   Mean   :0.4045  \n 3rd Qu.: 0.010545   3rd Qu.:0.52525   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   : 0.111111   Max.   :0.73196   Max.   :1.0000   Max.   :1.0000  \n NA's   :3           NA's   :35        NA's   :35       NA's   :35      \n     blue0            redcty          bluecty           pwhite       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00942  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.75584  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.87280  \n Mean   :0.5955   Mean   :0.5102   Mean   :0.4887   Mean   :0.81960  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.93883  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n NA's   :35       NA's   :105      NA's   :105      NA's   :1866     \n     pblack          page18_39        ave_hh_sz     median_hhincome \n Min.   :0.00000   Min.   :0.0000   Min.   :0.000   Min.   :  5000  \n 1st Qu.:0.01473   1st Qu.:0.2583   1st Qu.:2.210   1st Qu.: 39181  \n Median :0.03655   Median :0.3055   Median :2.440   Median : 50673  \n Mean   :0.08671   Mean   :0.3217   Mean   :2.429   Mean   : 54816  \n 3rd Qu.:0.09088   3rd Qu.:0.3691   3rd Qu.:2.660   3rd Qu.: 66005  \n Max.   :0.98962   Max.   :0.9975   Max.   :5.270   Max.   :200001  \n NA's   :2036      NA's   :1866     NA's   :1862    NA's   :1874    \n     powner        psch_atlstba    pop_propurban   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.5602   1st Qu.:0.2356   1st Qu.:0.8849  \n Median :0.7123   Median :0.3737   Median :1.0000  \n Mean   :0.6694   Mean   :0.3917   Mean   :0.8720  \n 3rd Qu.:0.8168   3rd Qu.:0.5300   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1869     NA's   :1868     NA's   :1866    \n\n\n\n\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\n\n\ndf %&gt;%\n    group_by(treatment) %&gt;%\n    summarise(\n    total_donors = sum(gave),\n    total_amount = sum(amount),\n    avg_amount = mean(amount[gave == 1], na.rm = TRUE),\n    response_rate = mean(gave)\n    )\n\n# A tibble: 2 × 5\n  treatment total_donors total_amount avg_amount response_rate\n      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1         0          298       13571        45.5        0.0179\n2         1          736       32290.       43.9        0.0220\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\n\n\n\nBalance Test\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# t-tests\nt.test(mrm2 ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  mrm2 by treatment\nt = -0.11953, df = 33394, p-value = 0.9049\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.2381015  0.2107298\nsample estimates:\nmean in group 0 mean in group 1 \n       12.99814        13.01183 \n\nt.test(female ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  female by treatment\nt = 1.7535, df = 32451, p-value = 0.07952\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.0008888548  0.0159826921\nsample estimates:\nmean in group 0 mean in group 1 \n      0.2826978       0.2751509 \n\nt.test(years ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  years by treatment\nt = 1.0909, df = 32401, p-value = 0.2753\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.04584866  0.16094698\nsample estimates:\nmean in group 0 mean in group 1 \n       6.135914        6.078365 \n\n\n\n\n\nsummary(lm(mrm2 ~ treatment, data = df))\n\n\nCall:\nlm(formula = mrm2 ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.012  -9.012  -5.012   6.002 154.988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.99814    0.09353 138.979   &lt;2e-16 ***\ntreatment    0.01369    0.11453   0.119    0.905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.08 on 50080 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  2.851e-07, Adjusted R-squared:  -1.968e-05 \nF-statistic: 0.01428 on 1 and 50080 DF,  p-value: 0.9049\n\nsummary(lm(female ~ treatment, data = df))\n\n\nCall:\nlm(formula = female ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2827 -0.2752 -0.2752  0.7173  0.7248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.282698   0.003504  80.688   &lt;2e-16 ***\ntreatment   -0.007547   0.004292  -1.758   0.0787 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4478 on 48970 degrees of freedom\n  (1111 observations deleted due to missingness)\nMultiple R-squared:  6.313e-05, Adjusted R-squared:  4.271e-05 \nF-statistic: 3.092 on 1 and 48970 DF,  p-value: 0.07869\n\nsummary(lm(years ~ treatment, data = df))\n\n\nCall:\nlm(formula = years ~ treatment, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.136 -4.136 -1.136  2.864 88.922 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.13591    0.04260 144.023   &lt;2e-16 ***\ntreatment   -0.05755    0.05217  -1.103     0.27    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.503 on 50080 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  2.429e-05, Adjusted R-squared:  4.327e-06 \nF-statistic: 1.217 on 1 and 50080 DF,  p-value: 0.27\n\n\n\n\n\n\nTo verify the randomization, we tested whether the treatment and control groups differ significantly on a few pre-treatment characteristics.\nWe compared: - Months since last donation (mrm2) - Gender (female) (female) - Years since first donation (years)\nWe did this both using Welch two-sample t-tests and simple linear regressions, regressing each variable on the treatment indicator.\n\n\n\nmrm2: p = 0.905 → no difference\nfemale: p = 0.080 → not significant at 95%, but somewhat close\nyears: p = 0.275 → no difference\n\n\n\n\n\nCoefficient on treatment for mrm2: 0.014 (p = 0.905)\nCoefficient on treatment for female: -0.0075 (p = 0.079)\nCoefficient on treatment for years: -0.058 (p = 0.27)\n\nThe results are consistent across both methods and match Table 1 in Karlan & List (2007) — there are no statistically significant differences, supporting the validity of the randomization process. This reassures us that any treatment effects we detect later are unlikely to be driven by pre-existing group differences.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharitable Contribution Made\n\n\n\n\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n# Barplot: Proportion who donated by treatment status\ndf %&gt;%\n    group_by(treatment) %&gt;%\n    summarise(response_rate = mean(gave)) %&gt;%\n    mutate(group = if_else(treatment == 1, \"Treatment\", \"Control\")) %&gt;%\n    ggplot(aes(x = group, y = response_rate, fill = group)) +\n    geom_col(width = 0.5) +\n    labs(\n    title = \"Proportion of Individuals Who Donated\",\n    x = \"Group\",\n    y = \"Response Rate\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# T-test on donation rates\nt.test(gave ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by treatment\nt = -3.2095, df = 36577, p-value = 0.001331\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.006733310 -0.001627399\nsample estimates:\nmean in group 0 mean in group 1 \n     0.01785821      0.02203857 \n\n# Linear regression (bivariate)\nsummary(lm(gave ~ treatment, data = df))\n\n\nCall:\nlm(formula = gave ~ treatment, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\n\n\nTo measure whether the matching offer increased the likelihood of giving, I compared the proportion of individuals who donated in the treatment vs. control groups.\nFrom a simple bar plot, the treatment group clearly had a higher donation rate.\nThen I ran both a t-test and a linear regression:\n\nT-Test: The difference in donation rates is statistically significant (p = 0.0013), with the treatment group donating about 0.42 percentage points more than the control group.\nRegression: A bivariate OLS confirms this — being in the treatment group is associated with a 0.00418 increase in probability of donating.\n\nTogether, these results suggest that people are more likely to donate when they’re told their donation will be matched. This aligns with behavioral economics — people may perceive greater value or urgency when their contribution is effectively “worth more.” Even though the absolute change is small, it’s meaningful in a large-scale campaign context.\n\n# Probit model: whether a donation was made ~ treatment assignment\nsummary(glm(gave ~ treatment, data = df, family = binomial(link = \"probit\")))\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\nTo model the probability of donation using a nonlinear link, I estimated a probit model with gave as the dependent variable and treatment as the independent variable.\nThe probit results show: - A positive and statistically significant coefficient for treatment (0.087, p = 0.0019), - Meaning individuals assigned to the matching offer were significantly more likely to donate.\nThis confirms the earlier t-test and OLS findings — and matches Table 3, Column 1 of the paper. The matching offer increased donation likelihood. While the numerical change is small, in large-scale campaigns even modest increases in donor response rates can translate to substantial revenue gains.\nOverall, this analysis shows that matching offers matter — not necessarily because they increase each individual’s donation size, but because they nudge more people to give in the first place.\n\n\n\n\n\n\n\n\n\n\nDifferences between Match Rates\n\n\n\n\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Filter treatment group only (exclude control)\ndf_ratio &lt;- df %&gt;% filter(ratio &gt; 0)\n\n# t-tests between match ratios\nt.test(gave ~ ratio, data = df_ratio %&gt;% filter(ratio %in% c(1, 2)))\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.005711275  0.001942773\nsample estimates:\nmean in group 1 mean in group 2 \n     0.02074912      0.02263338 \n\nt.test(gave ~ ratio, data = df_ratio %&gt;% filter(ratio %in% c(2, 3)))\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means between group 2 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.004012044  0.003811996\nsample estimates:\nmean in group 2 mean in group 3 \n     0.02263338      0.02273340 \n\nt.test(gave ~ ratio, data = df_ratio %&gt;% filter(ratio %in% c(1, 3)))\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -1.015, df = 22215, p-value = 0.3101\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.005816051  0.001847501\nsample estimates:\nmean in group 1 mean in group 3 \n     0.02074912      0.02273340 \n\n\n\n# Regression using indicator variables for match ratios\ndf_ratio &lt;- df_ratio %&gt;%\n    mutate(\n    ratio1 = if_else(ratio == 1, 1, 0),\n    ratio2 = if_else(ratio == 2, 1, 0),\n    ratio3 = if_else(ratio == 3, 1, 0)\n    )\n\nsummary(lm(gave ~ ratio1 + ratio2 + ratio3, data = df_ratio))\n\n\nCall:\nlm(formula = gave ~ ratio1 + ratio2 + ratio3, data = df_ratio)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02273 -0.02263 -0.02075  0.97925 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.022733   0.001392  16.335   &lt;2e-16 ***\nratio1      -0.001984   0.001968  -1.008    0.313    \nratio2      -0.000100   0.001968  -0.051    0.959    \nratio3             NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1468 on 33393 degrees of freedom\nMultiple R-squared:  3.865e-05, Adjusted R-squared:  -2.124e-05 \nF-statistic: 0.6454 on 2 and 33393 DF,  p-value: 0.5245\n\n\n\n# Response rates by ratio (filtered to treatment group)\ndf_ratio %&gt;%\n    group_by(ratio) %&gt;%\n    summarise(response_rate = mean(gave))\n\n# A tibble: 3 × 2\n  ratio     response_rate\n  &lt;dbl+lbl&gt;         &lt;dbl&gt;\n1 1                0.0207\n2 2                0.0226\n3 3                0.0227\n\n\n\n# Differences\n# 2:1 - 1:1\ndf_ratio %&gt;% filter(ratio == 2) %&gt;% summarise(mean(gave)) -\ndf_ratio %&gt;% filter(ratio == 1) %&gt;% summarise(mean(gave))\n\n   mean(gave)\n1 0.001884251\n\n# 3:1 - 2:1\ndf_ratio %&gt;% filter(ratio == 3) %&gt;% summarise(mean(gave)) -\ndf_ratio %&gt;% filter(ratio == 2) %&gt;% summarise(mean(gave))\n\n   mean(gave)\n1 0.000100024\n\n\n\n\n\n\n\n\n\n\n\nDifferences Between Match Rates\n\n\n\n\n\nNext, I assessed whether larger match ratios (e.g., $2:$1 or $3:$1) actually led to higher response rates compared to the standard $1:$1 match.\n\n\nI ran t-tests comparing the likelihood of donating between different match ratios. None of the differences were statistically significant at the 95% level: - $2:$1 vs $1:$1: p = 0.33 - $3:$1 vs $2:$1: p = 0.96 - $3:$1 vs $1:$1: p = 0.31\nThese results align with what Karlan & List (2007) state: “larger match ratios… had no additional impact.”\n\n\n\n\n\n\n\n\n\n\nRegression Results:\n\n\n\n\n\nI regressed gave on binary indicators for each match ratio. Using $3:$1 as the base group, both the $1:$1 and $2:$1 groups had slightly lower donation rates, but again, the differences were not statistically significant.\n\n$1:$1: –0.20 percentage points\n$2:$1: –0.01 percentage points\n\n\n\n\n\n\n\n\n\n\nDirect Comparison of Response Rates:\n\n\n\n\n\nFrom the raw data: - $1:$1 match: 2.07% response rate - $2:$1 match: 2.26% - $3:$1 match: 2.27%\nDifference between: - $2:$1 and $1:$1: +0.19 percentage points - $3:$1 and $2:$1: +0.01 percentage points\n\n\nWhile all match treatments increased donation rates over the control group, larger match ratios didn’t generate additional gains. This challenges the common intuition among fundraisers that a bigger match is always more compelling. Instead, it seems that the presence of a match, rather than its size, is what nudges behavior.\n\n\n\n\n\n\n\n\n\n\nSize of Charitable Contribution\n\n\n\n\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Linear regression on donation amount (includes zeros)\nsummary(lm(amount ~ treatment, data = df))\n\n\nCall:\nlm(formula = amount ~ treatment, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.97  -0.97  -0.97  -0.81 399.03 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.81327    0.06742  12.063   &lt;2e-16 ***\ntreatment    0.15361    0.08256   1.861   0.0628 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.709 on 50081 degrees of freedom\nMultiple R-squared:  6.911e-05, Adjusted R-squared:  4.915e-05 \nF-statistic: 3.461 on 1 and 50081 DF,  p-value: 0.06282\n\n\n\n# Limit to people who donated\ndf_donors &lt;- df %&gt;% filter(gave == 1)\n\n# Regression: amount ~ treatment (conditional on giving)\nsummary(lm(amount ~ treatment, data = df_donors))\n\n\nCall:\nlm(formula = amount ~ treatment, data = df_donors)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43.54 -23.87 -18.87   6.13 356.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.540      2.423  18.792   &lt;2e-16 ***\ntreatment     -1.668      2.872  -0.581    0.561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 1032 degrees of freedom\nMultiple R-squared:  0.0003268, Adjusted R-squared:  -0.0006419 \nF-statistic: 0.3374 on 1 and 1032 DF,  p-value: 0.5615\n\n\n\n\n\ndf_donors %&gt;%\n    filter(treatment == 0) %&gt;%\n    ggplot(aes(x = amount)) +\n    geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"white\") +\n    geom_vline(aes(xintercept = mean(amount)), color = \"red\", linetype = \"dashed\", size = 1) +\n    labs(title = \"Donation Distribution – Control Group\", x = \"Donation Amount\", y = \"Count\") +\n    theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_donors %&gt;%\n    filter(treatment == 1) %&gt;%\n    ggplot(aes(x = amount)) +\n    geom_histogram(binwidth = 5, fill = \"lightgreen\", color = \"white\") +\n    geom_vline(aes(xintercept = mean(amount)), color = \"red\", linetype = \"dashed\", size = 1) +\n    labs(title = \"Donation Distribution – Treatment Group\", x = \"Donation Amount\", y = \"Count\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n:::: ::: {.callout-note collapse=“true”} ## Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made. :::: :::: ::: {.callout-note collapse=“true”} ### Law of Large Numbers\n\nset.seed(123)\n\n# Simulate donations (1 = gave, 0 = did not give)\ncontrol_sim &lt;- rbinom(n = 100000, size = 1, prob = 0.018)\ntreatment_sim &lt;- rbinom(n = 100000, size = 1, prob = 0.022)\n\n# Take 10,000 random samples of differences\ndiffs &lt;- treatment_sim[1:10000] - control_sim[1:10000]\n\n# Compute cumulative average difference\ncum_avg_diff &lt;- cumsum(diffs) / seq_along(diffs)\n\n# Plot cumulative average\nlibrary(ggplot2)\ntibble(step = 1:10000, cum_avg = cum_avg_diff) %&gt;%\n    ggplot(aes(x = step, y = cum_avg)) +\n    geom_line(color = \"blue\") +\n    geom_hline(yintercept = 0.004, linetype = \"dashed\", color = \"red\", size = 1) +\n    labs(\n    title = \"Cumulative Average of Simulated Differences in Donation Rates\",\n    x = \"Number of Simulated Observations\",\n    y = \"Cumulative Average (Treatment - Control)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n:::: :::: ::: {.callout-note collapse=“true”} ### Central Limit Theorem\n\nset.seed(123)\n\n# Helper function to simulate one round of average difference\nsimulate_diff &lt;- function(n, reps = 1000) {\n    replicate(reps, {\n    control &lt;- rbinom(n = n, size = 1, prob = 0.018)\n    treatment &lt;- rbinom(n = n, size = 1, prob = 0.022)\n    mean(treatment) - mean(control)\n    })\n}\n\n# Run simulations for different sample sizes\ndiff_50 &lt;- simulate_diff(50)\ndiff_200 &lt;- simulate_diff(200)\ndiff_500 &lt;- simulate_diff(500)\ndiff_1000 &lt;- simulate_diff(1000)\n\n# Create combined data frame for plotting\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndiffs_df &lt;- bind_rows(\n    tibble(diff = diff_50, sample_size = \"n = 50\"),\n    tibble(diff = diff_200, sample_size = \"n = 200\"),\n    tibble(diff = diff_500, sample_size = \"n = 500\"),\n    tibble(diff = diff_1000, sample_size = \"n = 1000\")\n)\n\n# Plot histograms with larger bins\nggplot(diffs_df, aes(x = diff)) +\n    geom_histogram(binwidth = 0.002, fill = \"steelblue\", color = \"white\") +\n    facet_wrap(~sample_size, scales = \"free_y\") +\n    geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n    labs(\n    title = \"Sampling Distribution of Mean Differences (Treatment - Control)\",\n    x = \"Average Difference\",\n    y = \"Count\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n:::: :::: ::: {.callout-note collapse=“true”}"
  },
  {
    "objectID": "Blog/Project3/index.html#introduction",
    "href": "Blog/Project3/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results.\n\n\n\n\n\n\nData\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(haven)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the dataset\ndf &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Display structure of the data\nglimpse(df)\n\nRows: 50,083\nColumns: 51\n$ treatment          &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, …\n$ control            &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, …\n$ ratio              &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 1,…\n$ ratio2             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, …\n$ ratio3             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ size               &lt;dbl+lbl&gt; 0, 0, 3, 4, 2, 0, 1, 3, 4, 1, 4, 2, 0, 1, 0, 4,…\n$ size25             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ size50             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ size100            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sizeno             &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ ask                &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 3, 3, 2, 2, 1, 3, 0, 2, 0, 1,…\n$ askd1              &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ askd2              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ askd3              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ ask1               &lt;dbl&gt; 55, 25, 55, 55, 35, 95, 125, 75, 250, 150, 125, 25,…\n$ ask2               &lt;dbl&gt; 70, 35, 70, 70, 45, 120, 160, 95, 315, 190, 160, 35…\n$ ask3               &lt;dbl&gt; 85, 50, 85, 85, 55, 145, 190, 120, 375, 225, 190, 5…\n$ amount             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gave               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ amountchange       &lt;dbl&gt; -45, -25, -50, -25, -15, -45, -50, -65, -100, -125,…\n$ hpa                &lt;dbl&gt; 45, 25, 50, 50, 25, 90, 100, 65, 200, 125, 100, 5, …\n$ ltmedmra           &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, …\n$ freq               &lt;dbl&gt; 2, 2, 3, 15, 42, 20, 12, 13, 28, 4, 1, 1, 2, 80, 3,…\n$ years              &lt;dbl&gt; 4, 3, 2, 8, 95, 10, 8, 16, 19, 7, 3, 1, 6, 19, 3, 1…\n$ year5              &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, …\n$ mrm2               &lt;dbl&gt; 31, 5, 6, 1, 24, 3, 4, 4, 6, 35, 41, 8, 28, 15, 5, …\n$ dormant            &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ female             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ couple             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ state50one         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ nonlit             &lt;dbl&gt; 5, 0, 3, 1, 1, 0, 0, 4, 1, 4, 4, 1, 1, 4, 0, 3, 6, …\n$ cases              &lt;dbl&gt; 4, 2, 1, 2, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 1, 1, 2, …\n$ statecnt           &lt;dbl&gt; 4.5002995, 2.9822462, 9.6070213, 3.2814682, 2.30201…\n$ stateresponse      &lt;dbl&gt; 0.01994681, 0.02608696, 0.02304817, 0.02066869, 0.0…\n$ stateresponset     &lt;dbl&gt; 0.019502353, 0.027833002, 0.022158911, 0.024702653,…\n$ stateresponsec     &lt;dbl&gt; 0.020806242, 0.022494888, 0.024743512, 0.012681159,…\n$ stateresponsetminc &lt;dbl&gt; -0.001303889, 0.005338114, -0.002584601, 0.01202149…\n$ perbush            &lt;dbl&gt; 0.4900000, 0.4646465, 0.4081633, 0.4646465, 0.52525…\n$ close25            &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ red0               &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, …\n$ blue0              &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, …\n$ redcty             &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, …\n$ bluecty            &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ pwhite             &lt;dbl&gt; 0.4464934, NA, 0.9357064, 0.8883309, 0.7590141, 0.8…\n$ pblack             &lt;dbl&gt; 0.527769208, NA, 0.011948366, 0.010760401, 0.127420…\n$ page18_39          &lt;dbl&gt; 0.3175913, NA, 0.2761282, 0.2794118, 0.4423889, 0.3…\n$ ave_hh_sz          &lt;dbl&gt; 2.10, NA, 2.48, 2.65, 1.85, 2.92, 2.10, 2.47, 2.49,…\n$ median_hhincome    &lt;dbl&gt; 28517, NA, 51175, 79269, 40908, 61779, 54655, 14152…\n$ powner             &lt;dbl&gt; 0.4998072, NA, 0.7219406, 0.9204314, 0.4160721, 0.9…\n$ psch_atlstba       &lt;dbl&gt; 0.32452780, NA, 0.19266793, 0.41214216, 0.43996516,…\n$ pop_propurban      &lt;dbl&gt; 1.0000000, NA, 1.0000000, 1.0000000, 1.0000000, 0.9…\n\n# Display summary statistics\nsummary(df)\n\n   treatment         control           ratio           ratio2      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.6668   Mean   :0.3332   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     ratio3            size           size25           size50      \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :2.000   Median :0.0000   Median :0.0000  \n Mean   :0.2222   Mean   :1.667   Mean   :0.1667   Mean   :0.1666  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :4.000   Max.   :1.0000   Max.   :1.0000  \n                                                                   \n    size100           sizeno            ask            askd1       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.1667   Mean   :0.1667   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     askd2            askd3             ask1             ask2        \n Min.   :0.0000   Min.   :0.0000   Min.   :  25.0   Min.   :  35.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  35.0   1st Qu.:  45.00  \n Median :0.0000   Median :0.0000   Median :  45.0   Median :  60.00  \n Mean   :0.2223   Mean   :0.2222   Mean   :  71.5   Mean   :  91.79  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:  65.0   3rd Qu.:  85.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1500.0   Max.   :1875.00  \n                                                                     \n      ask3          amount              gave          amountchange       \n Min.   :  50   Min.   :  0.0000   Min.   :0.00000   Min.   :-200412.12  \n 1st Qu.:  55   1st Qu.:  0.0000   1st Qu.:0.00000   1st Qu.:    -50.00  \n Median :  70   Median :  0.0000   Median :0.00000   Median :    -30.00  \n Mean   : 111   Mean   :  0.9157   Mean   :0.02065   Mean   :    -52.67  \n 3rd Qu.: 100   3rd Qu.:  0.0000   3rd Qu.:0.00000   3rd Qu.:    -25.00  \n Max.   :2250   Max.   :400.0000   Max.   :1.00000   Max.   :    275.00  \n                                                                         \n      hpa             ltmedmra           freq             years       \n Min.   :   0.00   Min.   :0.0000   Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  30.00   1st Qu.:0.0000   1st Qu.:  2.000   1st Qu.: 2.000  \n Median :  45.00   Median :0.0000   Median :  4.000   Median : 5.000  \n Mean   :  59.38   Mean   :0.4937   Mean   :  8.039   Mean   : 6.098  \n 3rd Qu.:  60.00   3rd Qu.:1.0000   3rd Qu.: 10.000   3rd Qu.: 9.000  \n Max.   :1000.00   Max.   :1.0000   Max.   :218.000   Max.   :95.000  \n                                                      NA's   :1       \n     year5             mrm2           dormant           female      \n Min.   :0.0000   Min.   :  0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:  4.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :  8.00   Median :1.0000   Median :0.0000  \n Mean   :0.5088   Mean   : 13.01   Mean   :0.5235   Mean   :0.2777  \n 3rd Qu.:1.0000   3rd Qu.: 19.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :168.00   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :1                         NA's   :1111    \n     couple         state50one            nonlit          cases    \n Min.   :0.0000   Min.   :0.0000000   Min.   :0.000   Min.   :0.0  \n 1st Qu.:0.0000   1st Qu.:0.0000000   1st Qu.:1.000   1st Qu.:1.0  \n Median :0.0000   Median :0.0000000   Median :3.000   Median :1.0  \n Mean   :0.0919   Mean   :0.0009983   Mean   :2.474   Mean   :1.5  \n 3rd Qu.:0.0000   3rd Qu.:0.0000000   3rd Qu.:4.000   3rd Qu.:2.0  \n Max.   :1.0000   Max.   :1.0000000   Max.   :6.000   Max.   :4.0  \n NA's   :1148                         NA's   :452     NA's   :452  \n    statecnt         stateresponse     stateresponset    stateresponsec   \n Min.   : 0.001995   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.: 1.833234   1st Qu.:0.01816   1st Qu.:0.01849   1st Qu.:0.01286  \n Median : 3.538799   Median :0.01971   Median :0.02170   Median :0.01988  \n Mean   : 5.998820   Mean   :0.02063   Mean   :0.02199   Mean   :0.01772  \n 3rd Qu.: 9.607021   3rd Qu.:0.02305   3rd Qu.:0.02470   3rd Qu.:0.02081  \n Max.   :17.368841   Max.   :0.07692   Max.   :0.11111   Max.   :0.05263  \n                                                         NA's   :3        \n stateresponsetminc     perbush           close25            red0       \n Min.   :-0.047619   Min.   :0.09091   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.001388   1st Qu.:0.44444   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 0.001779   Median :0.48485   Median :0.0000   Median :0.0000  \n Mean   : 0.004273   Mean   :0.48794   Mean   :0.1857   Mean   :0.4045  \n 3rd Qu.: 0.010545   3rd Qu.:0.52525   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   : 0.111111   Max.   :0.73196   Max.   :1.0000   Max.   :1.0000  \n NA's   :3           NA's   :35        NA's   :35       NA's   :35      \n     blue0            redcty          bluecty           pwhite       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00942  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.75584  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.87280  \n Mean   :0.5955   Mean   :0.5102   Mean   :0.4887   Mean   :0.81960  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.93883  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n NA's   :35       NA's   :105      NA's   :105      NA's   :1866     \n     pblack          page18_39        ave_hh_sz     median_hhincome \n Min.   :0.00000   Min.   :0.0000   Min.   :0.000   Min.   :  5000  \n 1st Qu.:0.01473   1st Qu.:0.2583   1st Qu.:2.210   1st Qu.: 39181  \n Median :0.03655   Median :0.3055   Median :2.440   Median : 50673  \n Mean   :0.08671   Mean   :0.3217   Mean   :2.429   Mean   : 54816  \n 3rd Qu.:0.09088   3rd Qu.:0.3691   3rd Qu.:2.660   3rd Qu.: 66005  \n Max.   :0.98962   Max.   :0.9975   Max.   :5.270   Max.   :200001  \n NA's   :2036      NA's   :1866     NA's   :1862    NA's   :1874    \n     powner        psch_atlstba    pop_propurban   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.5602   1st Qu.:0.2356   1st Qu.:0.8849  \n Median :0.7123   Median :0.3737   Median :1.0000  \n Mean   :0.6694   Mean   :0.3917   Mean   :0.8720  \n 3rd Qu.:0.8168   3rd Qu.:0.5300   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1869     NA's   :1868     NA's   :1866    \n\n\n\n\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\n\n\ndf %&gt;%\n    group_by(treatment) %&gt;%\n    summarise(\n    total_donors = sum(gave),\n    total_amount = sum(amount),\n    avg_amount = mean(amount[gave == 1], na.rm = TRUE),\n    response_rate = mean(gave)\n    )\n\n# A tibble: 2 × 5\n  treatment total_donors total_amount avg_amount response_rate\n      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1         0          298       13571        45.5        0.0179\n2         1          736       32290.       43.9        0.0220\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\n\n\n\nBalance Test\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# t-tests\nt.test(mrm2 ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  mrm2 by treatment\nt = -0.11953, df = 33394, p-value = 0.9049\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.2381015  0.2107298\nsample estimates:\nmean in group 0 mean in group 1 \n       12.99814        13.01183 \n\nt.test(female ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  female by treatment\nt = 1.7535, df = 32451, p-value = 0.07952\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.0008888548  0.0159826921\nsample estimates:\nmean in group 0 mean in group 1 \n      0.2826978       0.2751509 \n\nt.test(years ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  years by treatment\nt = 1.0909, df = 32401, p-value = 0.2753\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.04584866  0.16094698\nsample estimates:\nmean in group 0 mean in group 1 \n       6.135914        6.078365 \n\n\n\n\n\nsummary(lm(mrm2 ~ treatment, data = df))\n\n\nCall:\nlm(formula = mrm2 ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.012  -9.012  -5.012   6.002 154.988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.99814    0.09353 138.979   &lt;2e-16 ***\ntreatment    0.01369    0.11453   0.119    0.905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.08 on 50080 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  2.851e-07, Adjusted R-squared:  -1.968e-05 \nF-statistic: 0.01428 on 1 and 50080 DF,  p-value: 0.9049\n\nsummary(lm(female ~ treatment, data = df))\n\n\nCall:\nlm(formula = female ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2827 -0.2752 -0.2752  0.7173  0.7248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.282698   0.003504  80.688   &lt;2e-16 ***\ntreatment   -0.007547   0.004292  -1.758   0.0787 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4478 on 48970 degrees of freedom\n  (1111 observations deleted due to missingness)\nMultiple R-squared:  6.313e-05, Adjusted R-squared:  4.271e-05 \nF-statistic: 3.092 on 1 and 48970 DF,  p-value: 0.07869\n\nsummary(lm(years ~ treatment, data = df))\n\n\nCall:\nlm(formula = years ~ treatment, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.136 -4.136 -1.136  2.864 88.922 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.13591    0.04260 144.023   &lt;2e-16 ***\ntreatment   -0.05755    0.05217  -1.103     0.27    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.503 on 50080 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  2.429e-05, Adjusted R-squared:  4.327e-06 \nF-statistic: 1.217 on 1 and 50080 DF,  p-value: 0.27\n\n\n\n\n\n\nTo verify the randomization, we tested whether the treatment and control groups differ significantly on a few pre-treatment characteristics.\nWe compared: - Months since last donation (mrm2) - Gender (female) (female) - Years since first donation (years)\nWe did this both using Welch two-sample t-tests and simple linear regressions, regressing each variable on the treatment indicator.\n\n\n\nmrm2: p = 0.905 → no difference\nfemale: p = 0.080 → not significant at 95%, but somewhat close\nyears: p = 0.275 → no difference\n\n\n\n\n\nCoefficient on treatment for mrm2: 0.014 (p = 0.905)\nCoefficient on treatment for female: -0.0075 (p = 0.079)\nCoefficient on treatment for years: -0.058 (p = 0.27)\n\nThe results are consistent across both methods and match Table 1 in Karlan & List (2007) — there are no statistically significant differences, supporting the validity of the randomization process. This reassures us that any treatment effects we detect later are unlikely to be driven by pre-existing group differences.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharitable Contribution Made\n\n\n\n\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n# Barplot: Proportion who donated by treatment status\ndf %&gt;%\n    group_by(treatment) %&gt;%\n    summarise(response_rate = mean(gave)) %&gt;%\n    mutate(group = if_else(treatment == 1, \"Treatment\", \"Control\")) %&gt;%\n    ggplot(aes(x = group, y = response_rate, fill = group)) +\n    geom_col(width = 0.5) +\n    labs(\n    title = \"Proportion of Individuals Who Donated\",\n    x = \"Group\",\n    y = \"Response Rate\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# T-test on donation rates\nt.test(gave ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by treatment\nt = -3.2095, df = 36577, p-value = 0.001331\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.006733310 -0.001627399\nsample estimates:\nmean in group 0 mean in group 1 \n     0.01785821      0.02203857 \n\n# Linear regression (bivariate)\nsummary(lm(gave ~ treatment, data = df))\n\n\nCall:\nlm(formula = gave ~ treatment, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\n\n\nTo measure whether the matching offer increased the likelihood of giving, I compared the proportion of individuals who donated in the treatment vs. control groups.\nFrom a simple bar plot, the treatment group clearly had a higher donation rate.\nThen I ran both a t-test and a linear regression:\n\nT-Test: The difference in donation rates is statistically significant (p = 0.0013), with the treatment group donating about 0.42 percentage points more than the control group.\nRegression: A bivariate OLS confirms this — being in the treatment group is associated with a 0.00418 increase in probability of donating.\n\nTogether, these results suggest that people are more likely to donate when they’re told their donation will be matched. This aligns with behavioral economics — people may perceive greater value or urgency when their contribution is effectively “worth more.” Even though the absolute change is small, it’s meaningful in a large-scale campaign context.\n\n# Probit model: whether a donation was made ~ treatment assignment\nsummary(glm(gave ~ treatment, data = df, family = binomial(link = \"probit\")))\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\nTo model the probability of donation using a nonlinear link, I estimated a probit model with gave as the dependent variable and treatment as the independent variable.\nThe probit results show: - A positive and statistically significant coefficient for treatment (0.087, p = 0.0019), - Meaning individuals assigned to the matching offer were significantly more likely to donate.\nThis confirms the earlier t-test and OLS findings — and matches Table 3, Column 1 of the paper. The matching offer increased donation likelihood. While the numerical change is small, in large-scale campaigns even modest increases in donor response rates can translate to substantial revenue gains.\nOverall, this analysis shows that matching offers matter — not necessarily because they increase each individual’s donation size, but because they nudge more people to give in the first place.\n\n\n\n\n\n\n\n\n\n\nDifferences between Match Rates\n\n\n\n\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Filter treatment group only (exclude control)\ndf_ratio &lt;- df %&gt;% filter(ratio &gt; 0)\n\n# t-tests between match ratios\nt.test(gave ~ ratio, data = df_ratio %&gt;% filter(ratio %in% c(1, 2)))\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.005711275  0.001942773\nsample estimates:\nmean in group 1 mean in group 2 \n     0.02074912      0.02263338 \n\nt.test(gave ~ ratio, data = df_ratio %&gt;% filter(ratio %in% c(2, 3)))\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means between group 2 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.004012044  0.003811996\nsample estimates:\nmean in group 2 mean in group 3 \n     0.02263338      0.02273340 \n\nt.test(gave ~ ratio, data = df_ratio %&gt;% filter(ratio %in% c(1, 3)))\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -1.015, df = 22215, p-value = 0.3101\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.005816051  0.001847501\nsample estimates:\nmean in group 1 mean in group 3 \n     0.02074912      0.02273340 \n\n\n\n# Regression using indicator variables for match ratios\ndf_ratio &lt;- df_ratio %&gt;%\n    mutate(\n    ratio1 = if_else(ratio == 1, 1, 0),\n    ratio2 = if_else(ratio == 2, 1, 0),\n    ratio3 = if_else(ratio == 3, 1, 0)\n    )\n\nsummary(lm(gave ~ ratio1 + ratio2 + ratio3, data = df_ratio))\n\n\nCall:\nlm(formula = gave ~ ratio1 + ratio2 + ratio3, data = df_ratio)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02273 -0.02263 -0.02075  0.97925 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.022733   0.001392  16.335   &lt;2e-16 ***\nratio1      -0.001984   0.001968  -1.008    0.313    \nratio2      -0.000100   0.001968  -0.051    0.959    \nratio3             NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1468 on 33393 degrees of freedom\nMultiple R-squared:  3.865e-05, Adjusted R-squared:  -2.124e-05 \nF-statistic: 0.6454 on 2 and 33393 DF,  p-value: 0.5245\n\n\n\n# Response rates by ratio (filtered to treatment group)\ndf_ratio %&gt;%\n    group_by(ratio) %&gt;%\n    summarise(response_rate = mean(gave))\n\n# A tibble: 3 × 2\n  ratio     response_rate\n  &lt;dbl+lbl&gt;         &lt;dbl&gt;\n1 1                0.0207\n2 2                0.0226\n3 3                0.0227\n\n\n\n# Differences\n# 2:1 - 1:1\ndf_ratio %&gt;% filter(ratio == 2) %&gt;% summarise(mean(gave)) -\ndf_ratio %&gt;% filter(ratio == 1) %&gt;% summarise(mean(gave))\n\n   mean(gave)\n1 0.001884251\n\n# 3:1 - 2:1\ndf_ratio %&gt;% filter(ratio == 3) %&gt;% summarise(mean(gave)) -\ndf_ratio %&gt;% filter(ratio == 2) %&gt;% summarise(mean(gave))\n\n   mean(gave)\n1 0.000100024\n\n\n\n\n\n\n\n\n\n\n\nDifferences Between Match Rates\n\n\n\n\n\nNext, I assessed whether larger match ratios (e.g., $2:$1 or $3:$1) actually led to higher response rates compared to the standard $1:$1 match.\n\n\nI ran t-tests comparing the likelihood of donating between different match ratios. None of the differences were statistically significant at the 95% level: - $2:$1 vs $1:$1: p = 0.33 - $3:$1 vs $2:$1: p = 0.96 - $3:$1 vs $1:$1: p = 0.31\nThese results align with what Karlan & List (2007) state: “larger match ratios… had no additional impact.”\n\n\n\n\n\n\n\n\n\n\nRegression Results:\n\n\n\n\n\nI regressed gave on binary indicators for each match ratio. Using $3:$1 as the base group, both the $1:$1 and $2:$1 groups had slightly lower donation rates, but again, the differences were not statistically significant.\n\n$1:$1: –0.20 percentage points\n$2:$1: –0.01 percentage points\n\n\n\n\n\n\n\n\n\n\nDirect Comparison of Response Rates:\n\n\n\n\n\nFrom the raw data: - $1:$1 match: 2.07% response rate - $2:$1 match: 2.26% - $3:$1 match: 2.27%\nDifference between: - $2:$1 and $1:$1: +0.19 percentage points - $3:$1 and $2:$1: +0.01 percentage points\n\n\nWhile all match treatments increased donation rates over the control group, larger match ratios didn’t generate additional gains. This challenges the common intuition among fundraisers that a bigger match is always more compelling. Instead, it seems that the presence of a match, rather than its size, is what nudges behavior.\n\n\n\n\n\n\n\n\n\n\nSize of Charitable Contribution\n\n\n\n\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Linear regression on donation amount (includes zeros)\nsummary(lm(amount ~ treatment, data = df))\n\n\nCall:\nlm(formula = amount ~ treatment, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.97  -0.97  -0.97  -0.81 399.03 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.81327    0.06742  12.063   &lt;2e-16 ***\ntreatment    0.15361    0.08256   1.861   0.0628 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.709 on 50081 degrees of freedom\nMultiple R-squared:  6.911e-05, Adjusted R-squared:  4.915e-05 \nF-statistic: 3.461 on 1 and 50081 DF,  p-value: 0.06282\n\n\n\n# Limit to people who donated\ndf_donors &lt;- df %&gt;% filter(gave == 1)\n\n# Regression: amount ~ treatment (conditional on giving)\nsummary(lm(amount ~ treatment, data = df_donors))\n\n\nCall:\nlm(formula = amount ~ treatment, data = df_donors)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43.54 -23.87 -18.87   6.13 356.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.540      2.423  18.792   &lt;2e-16 ***\ntreatment     -1.668      2.872  -0.581    0.561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 1032 degrees of freedom\nMultiple R-squared:  0.0003268, Adjusted R-squared:  -0.0006419 \nF-statistic: 0.3374 on 1 and 1032 DF,  p-value: 0.5615\n\n\n\n\n\ndf_donors %&gt;%\n    filter(treatment == 0) %&gt;%\n    ggplot(aes(x = amount)) +\n    geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"white\") +\n    geom_vline(aes(xintercept = mean(amount)), color = \"red\", linetype = \"dashed\", size = 1) +\n    labs(title = \"Donation Distribution – Control Group\", x = \"Donation Amount\", y = \"Count\") +\n    theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_donors %&gt;%\n    filter(treatment == 1) %&gt;%\n    ggplot(aes(x = amount)) +\n    geom_histogram(binwidth = 5, fill = \"lightgreen\", color = \"white\") +\n    geom_vline(aes(xintercept = mean(amount)), color = \"red\", linetype = \"dashed\", size = 1) +\n    labs(title = \"Donation Distribution – Treatment Group\", x = \"Donation Amount\", y = \"Count\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n:::: ::: {.callout-note collapse=“true”} ## Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made. :::: :::: ::: {.callout-note collapse=“true”} ### Law of Large Numbers\n\nset.seed(123)\n\n# Simulate donations (1 = gave, 0 = did not give)\ncontrol_sim &lt;- rbinom(n = 100000, size = 1, prob = 0.018)\ntreatment_sim &lt;- rbinom(n = 100000, size = 1, prob = 0.022)\n\n# Take 10,000 random samples of differences\ndiffs &lt;- treatment_sim[1:10000] - control_sim[1:10000]\n\n# Compute cumulative average difference\ncum_avg_diff &lt;- cumsum(diffs) / seq_along(diffs)\n\n# Plot cumulative average\nlibrary(ggplot2)\ntibble(step = 1:10000, cum_avg = cum_avg_diff) %&gt;%\n    ggplot(aes(x = step, y = cum_avg)) +\n    geom_line(color = \"blue\") +\n    geom_hline(yintercept = 0.004, linetype = \"dashed\", color = \"red\", size = 1) +\n    labs(\n    title = \"Cumulative Average of Simulated Differences in Donation Rates\",\n    x = \"Number of Simulated Observations\",\n    y = \"Cumulative Average (Treatment - Control)\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n:::: :::: ::: {.callout-note collapse=“true”} ### Central Limit Theorem\n\nset.seed(123)\n\n# Helper function to simulate one round of average difference\nsimulate_diff &lt;- function(n, reps = 1000) {\n    replicate(reps, {\n    control &lt;- rbinom(n = n, size = 1, prob = 0.018)\n    treatment &lt;- rbinom(n = n, size = 1, prob = 0.022)\n    mean(treatment) - mean(control)\n    })\n}\n\n# Run simulations for different sample sizes\ndiff_50 &lt;- simulate_diff(50)\ndiff_200 &lt;- simulate_diff(200)\ndiff_500 &lt;- simulate_diff(500)\ndiff_1000 &lt;- simulate_diff(1000)\n\n# Create combined data frame for plotting\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndiffs_df &lt;- bind_rows(\n    tibble(diff = diff_50, sample_size = \"n = 50\"),\n    tibble(diff = diff_200, sample_size = \"n = 200\"),\n    tibble(diff = diff_500, sample_size = \"n = 500\"),\n    tibble(diff = diff_1000, sample_size = \"n = 1000\")\n)\n\n# Plot histograms with larger bins\nggplot(diffs_df, aes(x = diff)) +\n    geom_histogram(binwidth = 0.002, fill = \"steelblue\", color = \"white\") +\n    facet_wrap(~sample_size, scales = \"free_y\") +\n    geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n    labs(\n    title = \"Sampling Distribution of Mean Differences (Treatment - Control)\",\n    x = \"Average Difference\",\n    y = \"Count\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n:::: :::: ::: {.callout-note collapse=“true”}"
  },
  {
    "objectID": "Blog/Project3/index.html#data",
    "href": "Blog/Project3/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\n# Load necessary libraries\nlibrary(haven)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the dataset\ndf &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Display structure of the data\nglimpse(df)\n\nRows: 50,083\nColumns: 51\n$ treatment          &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, …\n$ control            &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, …\n$ ratio              &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 1,…\n$ ratio2             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, …\n$ ratio3             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ size               &lt;dbl+lbl&gt; 0, 0, 3, 4, 2, 0, 1, 3, 4, 1, 4, 2, 0, 1, 0, 4,…\n$ size25             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ size50             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ size100            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sizeno             &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ ask                &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 3, 3, 2, 2, 1, 3, 0, 2, 0, 1,…\n$ askd1              &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ askd2              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ askd3              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ ask1               &lt;dbl&gt; 55, 25, 55, 55, 35, 95, 125, 75, 250, 150, 125, 25,…\n$ ask2               &lt;dbl&gt; 70, 35, 70, 70, 45, 120, 160, 95, 315, 190, 160, 35…\n$ ask3               &lt;dbl&gt; 85, 50, 85, 85, 55, 145, 190, 120, 375, 225, 190, 5…\n$ amount             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gave               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ amountchange       &lt;dbl&gt; -45, -25, -50, -25, -15, -45, -50, -65, -100, -125,…\n$ hpa                &lt;dbl&gt; 45, 25, 50, 50, 25, 90, 100, 65, 200, 125, 100, 5, …\n$ ltmedmra           &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, …\n$ freq               &lt;dbl&gt; 2, 2, 3, 15, 42, 20, 12, 13, 28, 4, 1, 1, 2, 80, 3,…\n$ years              &lt;dbl&gt; 4, 3, 2, 8, 95, 10, 8, 16, 19, 7, 3, 1, 6, 19, 3, 1…\n$ year5              &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, …\n$ mrm2               &lt;dbl&gt; 31, 5, 6, 1, 24, 3, 4, 4, 6, 35, 41, 8, 28, 15, 5, …\n$ dormant            &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ female             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ couple             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ state50one         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ nonlit             &lt;dbl&gt; 5, 0, 3, 1, 1, 0, 0, 4, 1, 4, 4, 1, 1, 4, 0, 3, 6, …\n$ cases              &lt;dbl&gt; 4, 2, 1, 2, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 1, 1, 2, …\n$ statecnt           &lt;dbl&gt; 4.5002995, 2.9822462, 9.6070213, 3.2814682, 2.30201…\n$ stateresponse      &lt;dbl&gt; 0.01994681, 0.02608696, 0.02304817, 0.02066869, 0.0…\n$ stateresponset     &lt;dbl&gt; 0.019502353, 0.027833002, 0.022158911, 0.024702653,…\n$ stateresponsec     &lt;dbl&gt; 0.020806242, 0.022494888, 0.024743512, 0.012681159,…\n$ stateresponsetminc &lt;dbl&gt; -0.001303889, 0.005338114, -0.002584601, 0.01202149…\n$ perbush            &lt;dbl&gt; 0.4900000, 0.4646465, 0.4081633, 0.4646465, 0.52525…\n$ close25            &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ red0               &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, …\n$ blue0              &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, …\n$ redcty             &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, …\n$ bluecty            &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ pwhite             &lt;dbl&gt; 0.4464934, NA, 0.9357064, 0.8883309, 0.7590141, 0.8…\n$ pblack             &lt;dbl&gt; 0.527769208, NA, 0.011948366, 0.010760401, 0.127420…\n$ page18_39          &lt;dbl&gt; 0.3175913, NA, 0.2761282, 0.2794118, 0.4423889, 0.3…\n$ ave_hh_sz          &lt;dbl&gt; 2.10, NA, 2.48, 2.65, 1.85, 2.92, 2.10, 2.47, 2.49,…\n$ median_hhincome    &lt;dbl&gt; 28517, NA, 51175, 79269, 40908, 61779, 54655, 14152…\n$ powner             &lt;dbl&gt; 0.4998072, NA, 0.7219406, 0.9204314, 0.4160721, 0.9…\n$ psch_atlstba       &lt;dbl&gt; 0.32452780, NA, 0.19266793, 0.41214216, 0.43996516,…\n$ pop_propurban      &lt;dbl&gt; 1.0000000, NA, 1.0000000, 1.0000000, 1.0000000, 0.9…\n\n# Display summary statistics\nsummary(df)\n\n   treatment         control           ratio           ratio2      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.6668   Mean   :0.3332   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     ratio3            size           size25           size50      \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :2.000   Median :0.0000   Median :0.0000  \n Mean   :0.2222   Mean   :1.667   Mean   :0.1667   Mean   :0.1666  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :4.000   Max.   :1.0000   Max.   :1.0000  \n                                                                   \n    size100           sizeno            ask            askd1       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.1667   Mean   :0.1667   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     askd2            askd3             ask1             ask2        \n Min.   :0.0000   Min.   :0.0000   Min.   :  25.0   Min.   :  35.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  35.0   1st Qu.:  45.00  \n Median :0.0000   Median :0.0000   Median :  45.0   Median :  60.00  \n Mean   :0.2223   Mean   :0.2222   Mean   :  71.5   Mean   :  91.79  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:  65.0   3rd Qu.:  85.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1500.0   Max.   :1875.00  \n                                                                     \n      ask3          amount              gave          amountchange       \n Min.   :  50   Min.   :  0.0000   Min.   :0.00000   Min.   :-200412.12  \n 1st Qu.:  55   1st Qu.:  0.0000   1st Qu.:0.00000   1st Qu.:    -50.00  \n Median :  70   Median :  0.0000   Median :0.00000   Median :    -30.00  \n Mean   : 111   Mean   :  0.9157   Mean   :0.02065   Mean   :    -52.67  \n 3rd Qu.: 100   3rd Qu.:  0.0000   3rd Qu.:0.00000   3rd Qu.:    -25.00  \n Max.   :2250   Max.   :400.0000   Max.   :1.00000   Max.   :    275.00  \n                                                                         \n      hpa             ltmedmra           freq             years       \n Min.   :   0.00   Min.   :0.0000   Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  30.00   1st Qu.:0.0000   1st Qu.:  2.000   1st Qu.: 2.000  \n Median :  45.00   Median :0.0000   Median :  4.000   Median : 5.000  \n Mean   :  59.38   Mean   :0.4937   Mean   :  8.039   Mean   : 6.098  \n 3rd Qu.:  60.00   3rd Qu.:1.0000   3rd Qu.: 10.000   3rd Qu.: 9.000  \n Max.   :1000.00   Max.   :1.0000   Max.   :218.000   Max.   :95.000  \n                                                      NA's   :1       \n     year5             mrm2           dormant           female      \n Min.   :0.0000   Min.   :  0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:  4.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :  8.00   Median :1.0000   Median :0.0000  \n Mean   :0.5088   Mean   : 13.01   Mean   :0.5235   Mean   :0.2777  \n 3rd Qu.:1.0000   3rd Qu.: 19.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :168.00   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :1                         NA's   :1111    \n     couple         state50one            nonlit          cases    \n Min.   :0.0000   Min.   :0.0000000   Min.   :0.000   Min.   :0.0  \n 1st Qu.:0.0000   1st Qu.:0.0000000   1st Qu.:1.000   1st Qu.:1.0  \n Median :0.0000   Median :0.0000000   Median :3.000   Median :1.0  \n Mean   :0.0919   Mean   :0.0009983   Mean   :2.474   Mean   :1.5  \n 3rd Qu.:0.0000   3rd Qu.:0.0000000   3rd Qu.:4.000   3rd Qu.:2.0  \n Max.   :1.0000   Max.   :1.0000000   Max.   :6.000   Max.   :4.0  \n NA's   :1148                         NA's   :452     NA's   :452  \n    statecnt         stateresponse     stateresponset    stateresponsec   \n Min.   : 0.001995   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.: 1.833234   1st Qu.:0.01816   1st Qu.:0.01849   1st Qu.:0.01286  \n Median : 3.538799   Median :0.01971   Median :0.02170   Median :0.01988  \n Mean   : 5.998820   Mean   :0.02063   Mean   :0.02199   Mean   :0.01772  \n 3rd Qu.: 9.607021   3rd Qu.:0.02305   3rd Qu.:0.02470   3rd Qu.:0.02081  \n Max.   :17.368841   Max.   :0.07692   Max.   :0.11111   Max.   :0.05263  \n                                                         NA's   :3        \n stateresponsetminc     perbush           close25            red0       \n Min.   :-0.047619   Min.   :0.09091   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.001388   1st Qu.:0.44444   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 0.001779   Median :0.48485   Median :0.0000   Median :0.0000  \n Mean   : 0.004273   Mean   :0.48794   Mean   :0.1857   Mean   :0.4045  \n 3rd Qu.: 0.010545   3rd Qu.:0.52525   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   : 0.111111   Max.   :0.73196   Max.   :1.0000   Max.   :1.0000  \n NA's   :3           NA's   :35        NA's   :35       NA's   :35      \n     blue0            redcty          bluecty           pwhite       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00942  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.75584  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.87280  \n Mean   :0.5955   Mean   :0.5102   Mean   :0.4887   Mean   :0.81960  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.93883  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n NA's   :35       NA's   :105      NA's   :105      NA's   :1866     \n     pblack          page18_39        ave_hh_sz     median_hhincome \n Min.   :0.00000   Min.   :0.0000   Min.   :0.000   Min.   :  5000  \n 1st Qu.:0.01473   1st Qu.:0.2583   1st Qu.:2.210   1st Qu.: 39181  \n Median :0.03655   Median :0.3055   Median :2.440   Median : 50673  \n Mean   :0.08671   Mean   :0.3217   Mean   :2.429   Mean   : 54816  \n 3rd Qu.:0.09088   3rd Qu.:0.3691   3rd Qu.:2.660   3rd Qu.: 66005  \n Max.   :0.98962   Max.   :0.9975   Max.   :5.270   Max.   :200001  \n NA's   :2036      NA's   :1866     NA's   :1862    NA's   :1874    \n     powner        psch_atlstba    pop_propurban   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.5602   1st Qu.:0.2356   1st Qu.:0.8849  \n Median :0.7123   Median :0.3737   Median :1.0000  \n Mean   :0.6694   Mean   :0.3917   Mean   :0.8720  \n 3rd Qu.:0.8168   3rd Qu.:0.5300   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1869     NA's   :1868     NA's   :1866    \n\n\n\nDescription\ntodo: Read the data into R/Python and describe the data # Summarize donation outcomes by treatment condition\n\ndf %&gt;%\n    group_by(treatment) %&gt;%\n    summarise(\n    total_donors = sum(gave),\n    total_amount = sum(amount),\n    avg_amount = mean(amount[gave == 1], na.rm = TRUE),\n    response_rate = mean(gave)\n    )\n\n# A tibble: 2 × 5\n  treatment total_donors total_amount avg_amount response_rate\n      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1         0          298       13571        45.5        0.0179\n2         1          736       32290.       43.9        0.0220\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\n# t-tests\nt.test(mrm2 ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  mrm2 by treatment\nt = -0.11953, df = 33394, p-value = 0.9049\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.2381015  0.2107298\nsample estimates:\nmean in group 0 mean in group 1 \n       12.99814        13.01183 \n\nt.test(female ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  female by treatment\nt = 1.7535, df = 32451, p-value = 0.07952\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.0008888548  0.0159826921\nsample estimates:\nmean in group 0 mean in group 1 \n      0.2826978       0.2751509 \n\nt.test(years ~ treatment, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  years by treatment\nt = 1.0909, df = 32401, p-value = 0.2753\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.04584866  0.16094698\nsample estimates:\nmean in group 0 mean in group 1 \n       6.135914        6.078365"
  },
  {
    "objectID": "Blog/Project3/index.html#experimental-results",
    "href": "Blog/Project3/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "To verify the randomization, we tested whether the treatment and control groups differ significantly on a few pre-treatment characteristics.\nWe compared: - Months since last donation (mrm2) - Gender (female) (female) - Years since first donation (years)\nWe did this both using Welch two-sample t-tests and simple linear regressions, regressing each variable on the treatment indicator.\n\n\n\nmrm2: p = 0.905 → no difference\nfemale: p = 0.080 → not significant at 95%, but somewhat close\nyears: p = 0.275 → no difference\n\n\n\n\n\nCoefficient on treatment for mrm2: 0.014 (p = 0.905)\nCoefficient on treatment for female: -0.0075 (p = 0.079)\nCoefficient on treatment for years: -0.058 (p = 0.27)\n\nThe results are consistent across both methods and match Table 1 in Karlan & List (2007) — there are no statistically significant differences, supporting the validity of the randomization process. This reassures us that any treatment effects we detect later are unlikely to be driven by pre-existing group differences."
  },
  {
    "objectID": "Blog/Project3/index.html#simulation-experiment",
    "href": "Blog/Project3/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\n\n\n\n\nLaw of Large Numbers\nTo demonstrate the Law of Large Numbers, I simulated 100,000 donation outcomes each for the control group (p = 0.018) and treatment group (p = 0.022). I then took 10,000 differences between matched treatment and control values and plotted the cumulative average of those differences over time.\nThe resulting line converges smoothly to 0.004, which is the true difference in population probabilities. This confirms that as the number of samples increases, the average of the observed differences converges to the expected difference — illustrating the Law of Large Numbers.\n\n\nCentral Limit Theorem\nNext, I explored how the distribution of the sample mean differences changes as sample size increases. For sample sizes 50, 200, 500, and 1000, I repeatedly took 1000 samples from each group, calculated the difference in means, and plotted histograms of those differences.\nWhat we observe: - For n = 50, the distribution is skewed and noisy. - By n = 200 and especially at n = 500 and 1000, the distributions are nearly normal and centered around the true difference (~0.004). - Most importantly, zero is in the tails, not the center, meaning we consistently detect a positive effect as sample size grows.\nThis is a clear visual example of the Central Limit Theorem: regardless of the underlying Bernoulli distribution, the sampling distribution of the difference in means becomes normal as n increases. ::::"
  },
  {
    "objectID": "Blog/Project3/index.html#simulation-experiment-1",
    "href": "Blog/Project3/index.html#simulation-experiment-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\n\nLaw of Large Numbers\nTo demonstrate the Law of Large Numbers, I simulated 100,000 donation outcomes each for the control group (p = 0.018) and treatment group (p = 0.022). I then took 10,000 differences between matched treatment and control values and plotted the cumulative average of those differences over time.\nThe resulting line converges smoothly to 0.004, which is the true difference in population probabilities. This confirms that as the number of samples increases, the average of the observed differences converges to the expected difference — illustrating the Law of Large Numbers.\n\n\nCentral Limit Theorem\nNext, I explored how the distribution of the sample mean differences changes as sample size increases. For sample sizes 50, 200, 500, and 1000, I repeatedly took 1000 samples from each group, calculated the difference in means, and plotted histograms of those differences.\nWhat we observe: - For n = 50, the distribution is skewed and noisy. - By n = 200 and especially at n = 500 and 1000, the distributions are nearly normal and centered around the true difference (~0.004). - Most importantly, zero is in the tails, not the center, meaning we consistently detect a positive effect as sample size grows.\nThis is a clear visual example of the Central Limit Theorem: regardless of the underlying Bernoulli distribution, the sampling distribution of the difference in means becomes normal as n increases."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Srujith Kapuluri",
    "section": "",
    "text": "Marketing(Data and CRM) Intern at UCSD Athletics"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "Blog/Project1/index.html",
    "href": "Blog/Project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data"
  },
  {
    "objectID": "Blog/Project1/index.html#section-1-data",
    "href": "Blog/Project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "Blog/Project1/index.html#section-2-analysis",
    "href": "Blog/Project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "Blog/Project2/index.html",
    "href": "Blog/Project2/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n    ggplot(aes(mpg, disp)) + \n    geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "Blog/Project2/index.html#sub-header",
    "href": "Blog/Project2/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n    ggplot(aes(mpg, disp)) + \n    geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "Blog/Project 4/hw2_questions.html",
    "href": "Blog/Project 4/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Load required library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the blueprinty dataset\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect structure and preview data\nstr(blueprinty)\n\nspc_tbl_ [1,500 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ patents   : num [1:1500] 0 3 4 3 3 6 5 5 6 4 ...\n $ region    : chr [1:1500] \"Midwest\" \"Southwest\" \"Northwest\" \"Northeast\" ...\n $ age       : num [1:1500] 32.5 37.5 27 24.5 37 29.5 27 20.5 25 29.5 ...\n $ iscustomer: num [1:1500] 0 0 1 0 0 1 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   patents = col_double(),\n  ..   region = col_character(),\n  ..   age = col_double(),\n  ..   iscustomer = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(blueprinty)\n\nRows: 1,500\nColumns: 4\n$ patents    &lt;dbl&gt; 0, 3, 4, 3, 3, 6, 5, 5, 6, 4, 2, 3, 7, 4, 5, 4, 2, 2, 2, 5,…\n$ region     &lt;chr&gt; \"Midwest\", \"Southwest\", \"Northwest\", \"Northeast\", \"Southwes…\n$ age        &lt;dbl&gt; 32.5, 37.5, 27.0, 24.5, 37.0, 29.5, 27.0, 20.5, 25.0, 29.5,…\n$ iscustomer &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,…\n\nsummary(blueprinty)\n\n    patents          region               age          iscustomer    \n Min.   : 0.000   Length:1500        Min.   : 9.00   Min.   :0.0000  \n 1st Qu.: 2.000   Class :character   1st Qu.:21.00   1st Qu.:0.0000  \n Median : 3.000   Mode  :character   Median :26.00   Median :0.0000  \n Mean   : 3.685                      Mean   :26.36   Mean   :0.3207  \n 3rd Qu.: 5.000                      3rd Qu.:31.62   3rd Qu.:1.0000  \n Max.   :16.000                      Max.   :49.00   Max.   :1.0000  \n\n\n\n# Split into customers and non-customers\ncustomers &lt;- blueprinty %&gt;% filter(iscustomer == 1)\nnon_customers &lt;- blueprinty %&gt;% filter(iscustomer == 0)\n\n# Compare summary stats\nsummary(customers$patents)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   2.000   4.000   4.133   6.000  16.000 \n\nsummary(non_customers$patents)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   2.000   3.000   3.473   5.000  16.000 \n\n# Optional: Visualize side-by-side\nggplot(blueprinty, aes(x = patents, fill = iscustomer)) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"white\") +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"), \n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Patent Distribution: Customer vs. Non-Customer\",\n       x = \"Number of Patents\", y = \"Frequency\", fill = \"Group\")\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# 1. Region distribution by customer status\nregion_table &lt;- table(blueprinty$region, blueprinty$iscustomer)\nprop.table(region_table, margin = 2)  # Proportions by group\n\n           \n                     0          1\n  Midwest   0.18351325 0.07692308\n  Northeast 0.26790972 0.68191268\n  Northwest 0.15505397 0.06029106\n  South     0.15309127 0.07276507\n  Southwest 0.24043180 0.10810811\n\nggplot(blueprinty, aes(x = region, fill = iscustomer)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"),\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Region by Customer Status\", y = \"Proportion\", fill = \"Group\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n# 2. Age distribution by customer status\nsummary(blueprinty %&gt;% group_by(iscustomer) %&gt;% summarise(avg_age = mean(age), sd_age = sd(age)))\n\n   iscustomer      avg_age         sd_age     \n Min.   :0.00   Min.   :26.1   Min.   :6.945  \n 1st Qu.:0.25   1st Qu.:26.3   1st Qu.:7.163  \n Median :0.50   Median :26.5   Median :7.380  \n Mean   :0.50   Mean   :26.5   Mean   :7.380  \n 3rd Qu.:0.75   3rd Qu.:26.7   3rd Qu.:7.597  \n Max.   :1.00   Max.   :26.9   Max.   :7.815  \n\nggplot(blueprinty, aes(x = iscustomer, y = age, fill = iscustomer)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\")) +\n  labs(title = \"Age Distribution by Customer Status\", x = \"Customer\", y = \"Age\") +\n  scale_x_discrete(labels = c(\"Non-Customer\", \"Customer\"))\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nIf each observation YiYi​ follows a Poisson distribution with parameter λλ, the likelihood function for a sample of nn independent observations Y1,Y2,…,YnY1​,Y2​,…,Yn​ is: L(λ;Y1,…,Yn)=∏i=1ne−λλYiYi! L(λ;Y1​,…,Yn​)=i=1∏n​Yi​!e−λλYi​​ 📌 Log-Likelihood Function:\nTaking the natural logarithm (which is easier for estimation), the log-likelihood becomes: log⁡L(λ)=∑i=1n[−λ+Yilog⁡(λ)−log⁡(Yi!)] logL(λ)=i=1∑n​[−λ+Yi​log(λ)−log(Yi​!)]\nThis is the foundation for estimating Poisson models via Maximum Likelihood Estimation (MLE).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "Blog/Project 4/hw2_questions.html#blueprinty-case-study",
    "href": "Blog/Project 4/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Load required library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the blueprinty dataset\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect structure and preview data\nstr(blueprinty)\n\nspc_tbl_ [1,500 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ patents   : num [1:1500] 0 3 4 3 3 6 5 5 6 4 ...\n $ region    : chr [1:1500] \"Midwest\" \"Southwest\" \"Northwest\" \"Northeast\" ...\n $ age       : num [1:1500] 32.5 37.5 27 24.5 37 29.5 27 20.5 25 29.5 ...\n $ iscustomer: num [1:1500] 0 0 1 0 0 1 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   patents = col_double(),\n  ..   region = col_character(),\n  ..   age = col_double(),\n  ..   iscustomer = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(blueprinty)\n\nRows: 1,500\nColumns: 4\n$ patents    &lt;dbl&gt; 0, 3, 4, 3, 3, 6, 5, 5, 6, 4, 2, 3, 7, 4, 5, 4, 2, 2, 2, 5,…\n$ region     &lt;chr&gt; \"Midwest\", \"Southwest\", \"Northwest\", \"Northeast\", \"Southwes…\n$ age        &lt;dbl&gt; 32.5, 37.5, 27.0, 24.5, 37.0, 29.5, 27.0, 20.5, 25.0, 29.5,…\n$ iscustomer &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,…\n\nsummary(blueprinty)\n\n    patents          region               age          iscustomer    \n Min.   : 0.000   Length:1500        Min.   : 9.00   Min.   :0.0000  \n 1st Qu.: 2.000   Class :character   1st Qu.:21.00   1st Qu.:0.0000  \n Median : 3.000   Mode  :character   Median :26.00   Median :0.0000  \n Mean   : 3.685                      Mean   :26.36   Mean   :0.3207  \n 3rd Qu.: 5.000                      3rd Qu.:31.62   3rd Qu.:1.0000  \n Max.   :16.000                      Max.   :49.00   Max.   :1.0000  \n\n\n\n# Split into customers and non-customers\ncustomers &lt;- blueprinty %&gt;% filter(iscustomer == 1)\nnon_customers &lt;- blueprinty %&gt;% filter(iscustomer == 0)\n\n# Compare summary stats\nsummary(customers$patents)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   2.000   4.000   4.133   6.000  16.000 \n\nsummary(non_customers$patents)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   2.000   3.000   3.473   5.000  16.000 \n\n# Optional: Visualize side-by-side\nggplot(blueprinty, aes(x = patents, fill = iscustomer)) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"white\") +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"), \n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Patent Distribution: Customer vs. Non-Customer\",\n       x = \"Number of Patents\", y = \"Frequency\", fill = \"Group\")\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# 1. Region distribution by customer status\nregion_table &lt;- table(blueprinty$region, blueprinty$iscustomer)\nprop.table(region_table, margin = 2)  # Proportions by group\n\n           \n                     0          1\n  Midwest   0.18351325 0.07692308\n  Northeast 0.26790972 0.68191268\n  Northwest 0.15505397 0.06029106\n  South     0.15309127 0.07276507\n  Southwest 0.24043180 0.10810811\n\nggplot(blueprinty, aes(x = region, fill = iscustomer)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"),\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Region by Customer Status\", y = \"Proportion\", fill = \"Group\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n# 2. Age distribution by customer status\nsummary(blueprinty %&gt;% group_by(iscustomer) %&gt;% summarise(avg_age = mean(age), sd_age = sd(age)))\n\n   iscustomer      avg_age         sd_age     \n Min.   :0.00   Min.   :26.1   Min.   :6.945  \n 1st Qu.:0.25   1st Qu.:26.3   1st Qu.:7.163  \n Median :0.50   Median :26.5   Median :7.380  \n Mean   :0.50   Mean   :26.5   Mean   :7.380  \n 3rd Qu.:0.75   3rd Qu.:26.7   3rd Qu.:7.597  \n Max.   :1.00   Max.   :26.9   Max.   :7.815  \n\nggplot(blueprinty, aes(x = iscustomer, y = age, fill = iscustomer)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\")) +\n  labs(title = \"Age Distribution by Customer Status\", x = \"Customer\", y = \"Age\") +\n  scale_x_discrete(labels = c(\"Non-Customer\", \"Customer\"))\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nIf each observation YiYi​ follows a Poisson distribution with parameter λλ, the likelihood function for a sample of nn independent observations Y1,Y2,…,YnY1​,Y2​,…,Yn​ is: L(λ;Y1,…,Yn)=∏i=1ne−λλYiYi! L(λ;Y1​,…,Yn​)=i=1∏n​Yi​!e−λλYi​​ 📌 Log-Likelihood Function:\nTaking the natural logarithm (which is easier for estimation), the log-likelihood becomes: log⁡L(λ)=∑i=1n[−λ+Yilog⁡(λ)−log⁡(Yi!)] logL(λ)=i=1∑n​[−λ+Yi​log(λ)−log(Yi​!)]\nThis is the foundation for estimating Poisson models via Maximum Likelihood Estimation (MLE).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "Blog/Project 4/hw2_questions.html#airbnb-case-study",
    "href": "Blog/Project 4/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "Blog/Project_4/hw2_questions.html",
    "href": "Blog/Project_4/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Load required library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the blueprinty dataset\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect structure and preview data\nstr(blueprinty)\n\nspc_tbl_ [1,500 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ patents   : num [1:1500] 0 3 4 3 3 6 5 5 6 4 ...\n $ region    : chr [1:1500] \"Midwest\" \"Southwest\" \"Northwest\" \"Northeast\" ...\n $ age       : num [1:1500] 32.5 37.5 27 24.5 37 29.5 27 20.5 25 29.5 ...\n $ iscustomer: num [1:1500] 0 0 1 0 0 1 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   patents = col_double(),\n  ..   region = col_character(),\n  ..   age = col_double(),\n  ..   iscustomer = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(blueprinty)\n\nRows: 1,500\nColumns: 4\n$ patents    &lt;dbl&gt; 0, 3, 4, 3, 3, 6, 5, 5, 6, 4, 2, 3, 7, 4, 5, 4, 2, 2, 2, 5,…\n$ region     &lt;chr&gt; \"Midwest\", \"Southwest\", \"Northwest\", \"Northeast\", \"Southwes…\n$ age        &lt;dbl&gt; 32.5, 37.5, 27.0, 24.5, 37.0, 29.5, 27.0, 20.5, 25.0, 29.5,…\n$ iscustomer &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,…\n\nsummary(blueprinty)\n\n    patents          region               age          iscustomer    \n Min.   : 0.000   Length:1500        Min.   : 9.00   Min.   :0.0000  \n 1st Qu.: 2.000   Class :character   1st Qu.:21.00   1st Qu.:0.0000  \n Median : 3.000   Mode  :character   Median :26.00   Median :0.0000  \n Mean   : 3.685                      Mean   :26.36   Mean   :0.3207  \n 3rd Qu.: 5.000                      3rd Qu.:31.62   3rd Qu.:1.0000  \n Max.   :16.000                      Max.   :49.00   Max.   :1.0000  \n\n\n\n# Histogram comparison\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\") +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"),\n                    name = \"Blueprinty Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  labs(title = \"Patent Count Distribution by Customer Status\",\n       x = \"Number of Patents\",\n       y = \"Count\")\n\n\n\n\n\n\n\n# Compare means\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents),\n            median_patents = median(patents),\n            sd_patents = sd(patents),\n            count = n())\n\n# A tibble: 2 × 5\n  iscustomer mean_patents median_patents sd_patents count\n       &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47              3       2.23  1019\n2          1         4.13              4       2.55   481\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Bar chart of regions by customer status\nggplot(blueprinty, aes(x = region, fill = factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"),\n                    name = \"Blueprinty Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  labs(title = \"Proportion of Firms in Each Region by Customer Status\",\n       x = \"Region\",\n       y = \"Proportion\")\n\n\n\n\n\n\n\n# Boxplot of age by customer status\nggplot(blueprinty, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"),\n                    name = \"Blueprinty Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  scale_x_discrete(labels = c(\"No\", \"Yes\")) +\n  labs(title = \"Age of Firms by Customer Status\",\n       x = \"Uses Blueprinty?\",\n       y = \"Firm Age\")\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet \\(Y_1, Y_2, \\dots, Y_n\\) be independent observations such that \\(Y_i \\sim \\text{Poisson}(\\lambda)\\).\nThen the likelihood function is: \\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\] Taking the natural logarithm, the log-likelihood becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\n\n# Define the log-likelihood function for Poisson\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)  # prevent log(0) or negative lambdas\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n\n# 1. Use the actual patent count vector from the data\nY &lt;- blueprinty$patents\n\n# 2. Define a range of lambda values\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\n# 3. Compute log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y))\n\n# 4. Plot\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     xlab = expression(lambda), ylab = \"Log-Likelihood\",\n     main = \"Log-Likelihood of Poisson Model\")\n\n\n\n\n\n\n\n\nLet \\(Y_1, Y_2, \\dots, Y_n \\overset{iid}{\\sim} \\text{Poisson}(\\lambda)\\), and recall that the log-likelihood function is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n= -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda + \\text{const}\n\\]\nTo find the MLE, we take the derivative with respect to \\(\\lambda\\) and set it equal to zero:\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\]\nSolving for \\(\\lambda\\) gives:\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense because the mean of a Poisson distribution is \\(\\lambda\\), so the sample mean \\(\\bar{Y}\\) is a natural estimator.\n\n# Define the negative log-likelihood (because optim() minimizes by default)\nneg_loglik &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(Inf)  # invalid lambda\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# Run optimization starting from a reasonable guess, e.g., mean(Y)\noptim_result &lt;- optim(par = mean(Y), fn = neg_loglik, Y = Y,\n                      method = \"Brent\", lower = 0.01, upper = 20)\n\n# Print the result\noptim_result$par  # this is the MLE of lambda\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Poisson regression log-likelihood function\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  # Compute lambda_i = exp(X %*% beta)\n  lambda &lt;- exp(X %*% beta)\n  \n  # Ensure all lambda values are positive and finite\n  if (any(lambda &lt;= 0 | !is.finite(lambda))) return(-Inf)\n\n  # Log-likelihood for Poisson regression\n  ll &lt;- sum(-lambda + Y * log(lambda) - lfactorial(Y))\n  return(ll)\n}\n\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(broom)\n\n# Create design matrix X\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(age2 = age^2,\n         iscustomer = as.numeric(as.character(iscustomer)))  # convert factor to numeric\n\n# Create region dummies (drop 1 for baseline)\nregion_dummies &lt;- model.matrix(~ region, data = blueprinty)[, -1]  # drop intercept and first region\n\n# Combine into design matrix X\nX &lt;- cbind(1, blueprinty$age, blueprinty$age2, region_dummies, blueprinty$iscustomer)\ncolnames(X)[1] &lt;- \"Intercept\"\n\n# Outcome vector\nY &lt;- blueprinty$patents\n\n# Define negative log-likelihood for use with optim()\nneg_loglik_reg &lt;- function(beta, Y, X) {\n  -poisson_regression_loglikelihood(beta, Y, X)\n}\n\n# Initial guess: 0s\ninit_beta &lt;- rep(0, ncol(X))\n\n# Optimize to find MLE\noptim_result &lt;- optim(par = init_beta,\n                      fn = neg_loglik_reg,\n                      Y = Y,\n                      X = X,\n                      method = \"BFGS\",\n                      hessian = TRUE)\n\n# Extract beta estimates and Hessian\nbeta_hat &lt;- optim_result$par\nhessian &lt;- optim_result$hessian\n\n# Compute standard errors: SE = sqrt(diag(inv(Hessian)))\nse_beta &lt;- sqrt(diag(solve(hessian)))\n\n# Create summary table\ncoef_table &lt;- tibble(\n  Variable = colnames(X),\n  Estimate = beta_hat,\n  Std_Error = se_beta\n)\n\nprint(coef_table)\n\n# A tibble: 8 × 3\n  Variable          Estimate Std_Error\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 \"Intercept\"       -0.126   0.112    \n2 \"\"                 0.116   0.00636  \n3 \"\"                -0.00223 0.0000771\n4 \"regionNortheast\" -0.0246  0.0434   \n5 \"regionNorthwest\" -0.0348  0.0529   \n6 \"regionSouth\"     -0.00544 0.0524   \n7 \"regionSouthwest\" -0.0378  0.0472   \n8 \"\"                 0.0607  0.0321   \n\n\n\n# Fit Poisson regression using glm()\nmodel_glm &lt;- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  data = blueprinty,\n  family = poisson(link = \"log\")\n)\n\n# View coefficient summary\nsummary(model_glm)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nCustomer effect is positive and significant → supports Blueprinty’s marketing claim\n\nAge relationship is nonlinear → makes sense, older firms may plateau or decline in innovation\n\nRegional differences matter — possibly due to local R&D incentives, tech concentration, etc.\n\n# Load libraries\nlibrary(tidyverse)\n\n# Step 1: Convert iscustomer to numeric 'customer'\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(customer = as.numeric(as.character(iscustomer)))\n\n# Step 2: Fit Poisson regression model\nglm_fit &lt;- glm(\n  patents ~ age + I(age^2) + region + customer,\n  data = blueprinty,\n  family = poisson(link = \"log\")\n)\n\n# Step 3: Extract beta coefficients\nbeta_hat &lt;- coef(glm_fit)\n\n# Step 4: Create design matrix with all firms set as non-customers (X_0)\nblueprinty_X0 &lt;- blueprinty %&gt;%\n  mutate(customer = 0)\nX_0 &lt;- model.matrix(~ age + I(age^2) + region + customer, data = blueprinty_X0)\n\n# Step 5: Create design matrix with all firms set as customers (X_1)\nblueprinty_X1 &lt;- blueprinty %&gt;%\n  mutate(customer = 1)\nX_1 &lt;- model.matrix(~ age + I(age^2) + region + customer, data = blueprinty_X1)\n\n# Step 6: Predict expected patent counts\ny_pred_0 &lt;- exp(X_0 %*% beta_hat)\ny_pred_1 &lt;- exp(X_1 %*% beta_hat)\n\n# Step 7: Estimate average treatment effect\ntreatment_effect &lt;- mean(y_pred_1 - y_pred_0)\n\n# Step 8: Output the result\ncat(\"✅ Estimated average treatment effect of Blueprinty software:\", round(treatment_effect, 3), \"additional patents per firm\\n\")\n\n✅ Estimated average treatment effect of Blueprinty software: 0.793 additional patents per firm"
  },
  {
    "objectID": "Blog/Project_4/hw2_questions.html#blueprinty-case-study",
    "href": "Blog/Project_4/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Load required library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the blueprinty dataset\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect structure and preview data\nstr(blueprinty)\n\nspc_tbl_ [1,500 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ patents   : num [1:1500] 0 3 4 3 3 6 5 5 6 4 ...\n $ region    : chr [1:1500] \"Midwest\" \"Southwest\" \"Northwest\" \"Northeast\" ...\n $ age       : num [1:1500] 32.5 37.5 27 24.5 37 29.5 27 20.5 25 29.5 ...\n $ iscustomer: num [1:1500] 0 0 1 0 0 1 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   patents = col_double(),\n  ..   region = col_character(),\n  ..   age = col_double(),\n  ..   iscustomer = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(blueprinty)\n\nRows: 1,500\nColumns: 4\n$ patents    &lt;dbl&gt; 0, 3, 4, 3, 3, 6, 5, 5, 6, 4, 2, 3, 7, 4, 5, 4, 2, 2, 2, 5,…\n$ region     &lt;chr&gt; \"Midwest\", \"Southwest\", \"Northwest\", \"Northeast\", \"Southwes…\n$ age        &lt;dbl&gt; 32.5, 37.5, 27.0, 24.5, 37.0, 29.5, 27.0, 20.5, 25.0, 29.5,…\n$ iscustomer &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,…\n\nsummary(blueprinty)\n\n    patents          region               age          iscustomer    \n Min.   : 0.000   Length:1500        Min.   : 9.00   Min.   :0.0000  \n 1st Qu.: 2.000   Class :character   1st Qu.:21.00   1st Qu.:0.0000  \n Median : 3.000   Mode  :character   Median :26.00   Median :0.0000  \n Mean   : 3.685                      Mean   :26.36   Mean   :0.3207  \n 3rd Qu.: 5.000                      3rd Qu.:31.62   3rd Qu.:1.0000  \n Max.   :16.000                      Max.   :49.00   Max.   :1.0000  \n\n\n\n# Histogram comparison\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\") +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"),\n                    name = \"Blueprinty Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  labs(title = \"Patent Count Distribution by Customer Status\",\n       x = \"Number of Patents\",\n       y = \"Count\")\n\n\n\n\n\n\n\n# Compare means\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents),\n            median_patents = median(patents),\n            sd_patents = sd(patents),\n            count = n())\n\n# A tibble: 2 × 5\n  iscustomer mean_patents median_patents sd_patents count\n       &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47              3       2.23  1019\n2          1         4.13              4       2.55   481\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Bar chart of regions by customer status\nggplot(blueprinty, aes(x = region, fill = factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"),\n                    name = \"Blueprinty Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  labs(title = \"Proportion of Firms in Each Region by Customer Status\",\n       x = \"Region\",\n       y = \"Proportion\")\n\n\n\n\n\n\n\n# Boxplot of age by customer status\nggplot(blueprinty, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"gray70\", \"steelblue\"),\n                    name = \"Blueprinty Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  scale_x_discrete(labels = c(\"No\", \"Yes\")) +\n  labs(title = \"Age of Firms by Customer Status\",\n       x = \"Uses Blueprinty?\",\n       y = \"Firm Age\")\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet \\(Y_1, Y_2, \\dots, Y_n\\) be independent observations such that \\(Y_i \\sim \\text{Poisson}(\\lambda)\\).\nThen the likelihood function is: \\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\] Taking the natural logarithm, the log-likelihood becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\n\n# Define the log-likelihood function for Poisson\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)  # prevent log(0) or negative lambdas\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n\n# 1. Use the actual patent count vector from the data\nY &lt;- blueprinty$patents\n\n# 2. Define a range of lambda values\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\n# 3. Compute log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y))\n\n# 4. Plot\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     xlab = expression(lambda), ylab = \"Log-Likelihood\",\n     main = \"Log-Likelihood of Poisson Model\")\n\n\n\n\n\n\n\n\nLet \\(Y_1, Y_2, \\dots, Y_n \\overset{iid}{\\sim} \\text{Poisson}(\\lambda)\\), and recall that the log-likelihood function is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n= -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda + \\text{const}\n\\]\nTo find the MLE, we take the derivative with respect to \\(\\lambda\\) and set it equal to zero:\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\]\nSolving for \\(\\lambda\\) gives:\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense because the mean of a Poisson distribution is \\(\\lambda\\), so the sample mean \\(\\bar{Y}\\) is a natural estimator.\n\n# Define the negative log-likelihood (because optim() minimizes by default)\nneg_loglik &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(Inf)  # invalid lambda\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# Run optimization starting from a reasonable guess, e.g., mean(Y)\noptim_result &lt;- optim(par = mean(Y), fn = neg_loglik, Y = Y,\n                      method = \"Brent\", lower = 0.01, upper = 20)\n\n# Print the result\noptim_result$par  # this is the MLE of lambda\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Poisson regression log-likelihood function\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  # Compute lambda_i = exp(X %*% beta)\n  lambda &lt;- exp(X %*% beta)\n  \n  # Ensure all lambda values are positive and finite\n  if (any(lambda &lt;= 0 | !is.finite(lambda))) return(-Inf)\n\n  # Log-likelihood for Poisson regression\n  ll &lt;- sum(-lambda + Y * log(lambda) - lfactorial(Y))\n  return(ll)\n}\n\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(broom)\n\n# Create design matrix X\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(age2 = age^2,\n         iscustomer = as.numeric(as.character(iscustomer)))  # convert factor to numeric\n\n# Create region dummies (drop 1 for baseline)\nregion_dummies &lt;- model.matrix(~ region, data = blueprinty)[, -1]  # drop intercept and first region\n\n# Combine into design matrix X\nX &lt;- cbind(1, blueprinty$age, blueprinty$age2, region_dummies, blueprinty$iscustomer)\ncolnames(X)[1] &lt;- \"Intercept\"\n\n# Outcome vector\nY &lt;- blueprinty$patents\n\n# Define negative log-likelihood for use with optim()\nneg_loglik_reg &lt;- function(beta, Y, X) {\n  -poisson_regression_loglikelihood(beta, Y, X)\n}\n\n# Initial guess: 0s\ninit_beta &lt;- rep(0, ncol(X))\n\n# Optimize to find MLE\noptim_result &lt;- optim(par = init_beta,\n                      fn = neg_loglik_reg,\n                      Y = Y,\n                      X = X,\n                      method = \"BFGS\",\n                      hessian = TRUE)\n\n# Extract beta estimates and Hessian\nbeta_hat &lt;- optim_result$par\nhessian &lt;- optim_result$hessian\n\n# Compute standard errors: SE = sqrt(diag(inv(Hessian)))\nse_beta &lt;- sqrt(diag(solve(hessian)))\n\n# Create summary table\ncoef_table &lt;- tibble(\n  Variable = colnames(X),\n  Estimate = beta_hat,\n  Std_Error = se_beta\n)\n\nprint(coef_table)\n\n# A tibble: 8 × 3\n  Variable          Estimate Std_Error\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 \"Intercept\"       -0.126   0.112    \n2 \"\"                 0.116   0.00636  \n3 \"\"                -0.00223 0.0000771\n4 \"regionNortheast\" -0.0246  0.0434   \n5 \"regionNorthwest\" -0.0348  0.0529   \n6 \"regionSouth\"     -0.00544 0.0524   \n7 \"regionSouthwest\" -0.0378  0.0472   \n8 \"\"                 0.0607  0.0321   \n\n\n\n# Fit Poisson regression using glm()\nmodel_glm &lt;- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  data = blueprinty,\n  family = poisson(link = \"log\")\n)\n\n# View coefficient summary\nsummary(model_glm)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nCustomer effect is positive and significant → supports Blueprinty’s marketing claim\n\nAge relationship is nonlinear → makes sense, older firms may plateau or decline in innovation\n\nRegional differences matter — possibly due to local R&D incentives, tech concentration, etc.\n\n# Load libraries\nlibrary(tidyverse)\n\n# Step 1: Convert iscustomer to numeric 'customer'\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(customer = as.numeric(as.character(iscustomer)))\n\n# Step 2: Fit Poisson regression model\nglm_fit &lt;- glm(\n  patents ~ age + I(age^2) + region + customer,\n  data = blueprinty,\n  family = poisson(link = \"log\")\n)\n\n# Step 3: Extract beta coefficients\nbeta_hat &lt;- coef(glm_fit)\n\n# Step 4: Create design matrix with all firms set as non-customers (X_0)\nblueprinty_X0 &lt;- blueprinty %&gt;%\n  mutate(customer = 0)\nX_0 &lt;- model.matrix(~ age + I(age^2) + region + customer, data = blueprinty_X0)\n\n# Step 5: Create design matrix with all firms set as customers (X_1)\nblueprinty_X1 &lt;- blueprinty %&gt;%\n  mutate(customer = 1)\nX_1 &lt;- model.matrix(~ age + I(age^2) + region + customer, data = blueprinty_X1)\n\n# Step 6: Predict expected patent counts\ny_pred_0 &lt;- exp(X_0 %*% beta_hat)\ny_pred_1 &lt;- exp(X_1 %*% beta_hat)\n\n# Step 7: Estimate average treatment effect\ntreatment_effect &lt;- mean(y_pred_1 - y_pred_0)\n\n# Step 8: Output the result\ncat(\"✅ Estimated average treatment effect of Blueprinty software:\", round(treatment_effect, 3), \"additional patents per firm\\n\")\n\n✅ Estimated average treatment effect of Blueprinty software: 0.793 additional patents per firm"
  },
  {
    "objectID": "Blog/Project_4/hw2_questions.html#airbnb-case-study",
    "href": "Blog/Project_4/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n# Load data\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n# Check structure\nglimpse(airbnb)\n\nRows: 40,628\nColumns: 14\n$ ...1                      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ id                        &lt;dbl&gt; 2515, 2595, 3647, 3831, 4611, 5099, 5107, 51…\n$ days                      &lt;dbl&gt; 3130, 3127, 3050, 3038, 3012, 2981, 2981, 29…\n$ last_scraped              &lt;chr&gt; \"4/2/2017\", \"4/2/2017\", \"4/2/2017\", \"4/2/201…\n$ host_since                &lt;chr&gt; \"9/6/2008\", \"9/9/2008\", \"11/25/2008\", \"12/7/…\n$ room_type                 &lt;chr&gt; \"Private room\", \"Entire home/apt\", \"Private …\n$ bathrooms                 &lt;dbl&gt; 1, 1, 1, 1, NA, 1, 1, NA, 1, 1, 1, 1, 1, NA,…\n$ bedrooms                  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ price                     &lt;dbl&gt; 59, 230, 150, 89, 39, 212, 250, 60, 129, 79,…\n$ number_of_reviews         &lt;dbl&gt; 150, 20, 0, 116, 93, 60, 60, 50, 53, 329, 11…\n$ review_scores_cleanliness &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 8, 9, 7, 10, 9, 9, 9,…\n$ review_scores_location    &lt;dbl&gt; 9, 10, NA, 9, 8, 9, 9, 9, 10, 10, 10, 9, 10,…\n$ review_scores_value       &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 9, 9, 9, 10, 9, 10, 9…\n$ instant_bookable          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FAL…\n\n# Drop rows with NA in key variables\nairbnb_clean &lt;- airbnb %&gt;%\n  select(number_of_reviews, room_type, bathrooms, bedrooms, price,\n         review_scores_cleanliness, review_scores_location,\n         review_scores_value, instant_bookable) %&gt;%\n  drop_na()\n\n# Convert categorical variables\nairbnb_clean &lt;- airbnb_clean %&gt;%\n  mutate(\n    room_type = factor(room_type),\n    instant_bookable = if_else(instant_bookable == \"t\", 1, 0)\n  )\n\n\n# Histogram of number_of_reviews\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\n  geom_histogram(binwidth = 5, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Number of Reviews\", x = \"Number of Reviews\", y = \"Count\")\n\n\n\n\n\n\n\n# Boxplot of reviews by room_type\nggplot(airbnb_clean, aes(x = room_type, y = number_of_reviews)) +\n  geom_boxplot(fill = \"lightgray\") +\n  labs(title = \"Number of Reviews by Room Type\")\n\n\n\n\n\n\n\n# Correlation with numeric variables\nairbnb_clean %&gt;%\n  select(number_of_reviews, price, review_scores_cleanliness, review_scores_location, review_scores_value) %&gt;%\n  cor(use = \"complete.obs\") %&gt;%\n  round(2)\n\n                          number_of_reviews price review_scores_cleanliness\nnumber_of_reviews                      1.00  0.00                      0.03\nprice                                  0.00  1.00                      0.03\nreview_scores_cleanliness              0.03  0.03                      1.00\nreview_scores_location                -0.05  0.10                      0.33\nreview_scores_value                   -0.03  0.00                      0.62\n                          review_scores_location review_scores_value\nnumber_of_reviews                          -0.05               -0.03\nprice                                       0.10                0.00\nreview_scores_cleanliness                   0.33                0.62\nreview_scores_location                      1.00                0.45\nreview_scores_value                         0.45                1.00\n\n\n\nggplot(airbnb_clean, aes(x = price, y = number_of_reviews)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  scale_x_log10() +\n  labs(title = \"Number of Reviews vs. Price\",\n       x = \"Price (log scale)\",\n       y = \"Number of Reviews\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Fit model\npoisson_model &lt;- glm(\n  number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n    review_scores_cleanliness + review_scores_location + review_scores_value +\n    instant_bookable,\n  data = airbnb_clean,\n  family = poisson(link = \"log\")\n)\n\n# View summary\nsummary(poisson_model)\n\n\nCall:\nglm(formula = number_of_reviews ~ room_type + bathrooms + bedrooms + \n    price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, family = poisson(link = \"log\"), \n    data = airbnb_clean)\n\nCoefficients: (1 not defined because of singularities)\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.714e+00  1.587e-02 233.976  &lt; 2e-16 ***\nroom_typePrivate room      7.405e-03  2.734e-03   2.709 0.006747 ** \nroom_typeShared room      -2.262e-01  8.616e-03 -26.249  &lt; 2e-16 ***\nbathrooms                 -1.164e-01  3.786e-03 -30.751  &lt; 2e-16 ***\nbedrooms                   7.607e-02  2.001e-03  38.016  &lt; 2e-16 ***\nprice                     -3.275e-05  8.521e-06  -3.843 0.000121 ***\nreview_scores_cleanliness  1.140e-01  1.486e-03  76.750  &lt; 2e-16 ***\nreview_scores_location    -8.065e-02  1.599e-03 -50.445  &lt; 2e-16 ***\nreview_scores_value       -9.749e-02  1.789e-03 -54.484  &lt; 2e-16 ***\ninstant_bookable                  NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 949198  on 30151  degrees of freedom\nAIC: 1070683\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n# Exponentiate coefficients\nexp(coef(poisson_model))\n\n              (Intercept)     room_typePrivate room      room_typeShared room \n               41.0024795                 1.0074329                 0.7975913 \n                bathrooms                  bedrooms                     price \n                0.8900915                 1.0790332                 0.9999673 \nreview_scores_cleanliness    review_scores_location       review_scores_value \n                1.1207975                 0.9225131                 0.9071082 \n         instant_bookable \n                       NA"
  }
]